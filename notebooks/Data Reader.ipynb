{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as D\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import concurrent.futures\n",
    "\n",
    "from functools import lru_cache\n",
    "from collections import OrderedDict\n",
    "from types import SimpleNamespace\n",
    "from collections.abc import Iterable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pytorch_transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Schemas(object):\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath) as f:\n",
    "            self.index = {}\n",
    "            for schema in json.load(f):\n",
    "                service_name = schema[\"service_name\"]\n",
    "                self.index[service_name] = schema\n",
    "\n",
    "    def get_service_desc(self, service):\n",
    "        return self.index[service][\"description\"]\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_slot_desc(self, service, slot):\n",
    "        for item in self.index[service][\"slots\"]:\n",
    "            if item[\"name\"] == slot:\n",
    "                return item[\"description\"]\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_intent_desc(self, service, intent):\n",
    "        for item in self.index[service][\"intents\"]:\n",
    "            if item[\"name\"] == intent:\n",
    "                return item[\"description\"]\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get(self, service):\n",
    "        result = dict(\n",
    "            # service\n",
    "            service_name=service,\n",
    "            service_desc=self.index[service][\"description\"],\n",
    "            \n",
    "            # slots\n",
    "            slot_name=[],\n",
    "            slot_desc=[],\n",
    "            slot_iscat=[], \n",
    "            slot_vals=[], # collected only for cat slots.. not sure if that makes sense\n",
    "\n",
    "            # intents\n",
    "            intent_name=[],\n",
    "            intent_desc=[],\n",
    "            intent_istrans=[],\n",
    "            intent_reqslots=[],\n",
    "            intent_optslots=[],\n",
    "            intent_optvals=[],\n",
    "        )\n",
    "\n",
    "        for slot in self.index[service][\"slots\"]:\n",
    "            result[\"slot_name\"].append(slot[\"name\"])\n",
    "            result[\"slot_desc\"].append(slot[\"description\"])\n",
    "            result[\"slot_iscat\"].append(slot[\"is_categorical\"])\n",
    "            result[\"slot_vals\"].append(slot[\"possible_values\"])\n",
    "        \n",
    "        for intent in self.index[service][\"intents\"]:\n",
    "            result[\"intent_name\"].append(intent[\"name\"])\n",
    "            result[\"intent_desc\"].append(intent[\"description\"])\n",
    "            result[\"intent_istrans\"].append(intent[\"is_transactional\"])\n",
    "            result[\"intent_reqslots\"].append(intent[\"required_slots\"])\n",
    "            result[\"intent_optslots\"].append(list(intent[\"optional_slots\"].keys()))\n",
    "            result[\"intent_optvals\"].append(list(intent[\"optional_slots\"].values()))\n",
    "\n",
    "        return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \n",
    "    def __init__(self, specials=[\"PAD\", \"SOS\", \"EOS\", \"UNK\"]):\n",
    "        self.token2index = OrderedDict()\n",
    "        self.index2token = OrderedDict()\n",
    "        for s in specials:\n",
    "            index = self.add(s)\n",
    "            setattr(self, s, index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token2index)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for index, token in self.index2token.items():\n",
    "            yield index, token\n",
    "            \n",
    "    def get_token(self, index):\n",
    "        return self.index2token.get(index)\n",
    "    \n",
    "    def get_index(self, token):\n",
    "        return self.token2index.get(token)\n",
    "    \n",
    "    def add(self, token):\n",
    "        if token not in self.token2index:\n",
    "            index = len(self.token2index)\n",
    "            self.token2index[token] = index\n",
    "            self.index2token[index] = token\n",
    "            return index\n",
    "        \n",
    "    def save(self, filename):\n",
    "        with open(filename, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f, delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "            for index, token in self:\n",
    "                writer.writerow([index, token])\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        vocab = cls(specials=[])\n",
    "        with open(filename, newline=\"\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "            for index, token in reader:\n",
    "                vocab.add(token)\n",
    "                assert vocab.get_index(token) == int(index)\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        self.bert = bert\n",
    "        \n",
    "    def __call__(self, text, include_sos=True):\n",
    "        tokens = self.bert.tokenize(text)\n",
    "        if include_sos:\n",
    "            tokens.insert(0, \"[CLS]\")\n",
    "            tokens.append(\"[SEP]\")\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "class TokenIndexer:\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        self.bert = bert\n",
    "        \n",
    "    def __call__(self, *args, **kw):\n",
    "        return self.bert.convert_tokens_to_ids(*args, **kw)\n",
    "    \n",
    "    \n",
    "bert_ = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = Tokenizer(bert_)\n",
    "token_indexer = TokenIndexer(bert_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def label_binarize(labels, classes):\n",
    "    # labels: np.array or tensor [batch, classes]\n",
    "    # classes: [..] list of classes\n",
    "    # weirdly,`sklearn.preprocessing.label_binarize` returns [1] or [0]\n",
    "    # instead of onehot ONLY when executing in this script!\n",
    "    vectors = [np.zeros(len(classes)) for _ in classes]\n",
    "    for i, label in enumerate(labels):\n",
    "        for j, c in enumerate(classes):\n",
    "            if c == label:\n",
    "                vectors[i][j] = 1\n",
    "    return np.array(vectors)\n",
    "    \n",
    "\n",
    "def label_inv_binarize(vectors, classes):\n",
    "    # labels: np.array or tensor [batch, classes]\n",
    "    # classes: [..] list of classes\n",
    "    # follows sklearn LabelBinarizer.inverse_transform()\n",
    "    # given all zeros, predicts label at index 0, instead of returning none!\n",
    "    # sklearn doesn't have functional API of inverse transform\n",
    "    labels = []\n",
    "    for each in vectors:\n",
    "        index = np.argmax(each)\n",
    "        labels.append(classes[index])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def padded_array(array, value=0):\n",
    "    # TODO: this does not do type checking.\n",
    "    # expects array to have fixed _number_ of dimensions\n",
    "    # resolve the shape of padded array\n",
    "    shape_index = {}\n",
    "    queue = [(array, 0)]\n",
    "    while queue:\n",
    "        subarr, dim = queue.pop(0)\n",
    "        shape_index[dim] = max(shape_index.get(dim, -1), len(subarr))\n",
    "        for x in subarr:\n",
    "            if isinstance(x, Iterable):\n",
    "                queue.append((x, dim+1))\n",
    "    shape = [shape_index[k] for k in range(max(shape_index) + 1)]\n",
    "\n",
    "    padded = np.ones(shape) * value\n",
    "    queue = [(array, [])]\n",
    "    while queue:\n",
    "        subarr, index = queue.pop(0)\n",
    "        for j, x in enumerate(subarr):\n",
    "            if isinstance(x, Iterable):\n",
    "                queue.append((x, index + [j]))\n",
    "            else:\n",
    "                padded[tuple(index + [j])] = x\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueDataset(D.Dataset):\n",
    "\n",
    "    def __init__(self, filename, schemas, tokenizer, token_indexer):\n",
    "        with open(filename) as f:\n",
    "            self.ds = json.load(f)\n",
    "        self.schemas = schemas\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexer = token_indexer\n",
    "        self.dialogues = []\n",
    "        self.default_padding = 0\n",
    "        for dial in self.ds:\n",
    "            fields = self.text_to_fields(dial)\n",
    "            self.dialogues.append(fields)\n",
    "        # cant' pickle these, and not required too\n",
    "        self.tokenizer = None\n",
    "        self.token_indexer = None\n",
    "        self.schemas = None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dialogues[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def field_dialogue_id(self, dialogue):\n",
    "        return {\"value\": dialogue[\"dialogue_id\"]}\n",
    "\n",
    "    def field_turn_speaker(self, turnid, dialogue):\n",
    "        return {\"value\": dialogue[\"turns\"][turnid][\"speaker\"]}\n",
    "    \n",
    "    def field_turn_utter(self, turnid, dialogue):\n",
    "        text = dialogue[\"turns\"][turnid][\"utterance\"]\n",
    "        tokens = self.tokenizer(text)\n",
    "        token_indices = self.token_indexer(tokens)\n",
    "        token_mask = [1] * len(tokens)\n",
    "        return {\"value\": text, \"tokens\": tokens, \"ids\": token_indices, \"mask\": token_mask}\n",
    "    \n",
    "    def field_turn_sys_utter(self, turnid, dialogue):\n",
    "        turn = dialogue[\"turns\"][turnid]\n",
    "        if turn[\"speaker\"] == \"SYSTEM\":\n",
    "            return self.field_turn_utter(turnid, dialogue)\n",
    "        \n",
    "    def field_turn_usr_utter(self, turnid, dialogue):\n",
    "        turn = dialogue[\"turns\"][turnid]\n",
    "        if turn[\"speaker\"] == \"USER\":\n",
    "            return self.field_turn_utter(turnid, dialogue)\n",
    "\n",
    "    def field_service(self, dialogue):\n",
    "        return {\"value\": dialogue[\"services\"]}\n",
    "\n",
    "    def field_service_desc(self, dialogue):\n",
    "        resp = dict(\n",
    "            value=[],\n",
    "            tokens=[],\n",
    "            ids=[],\n",
    "            mask=[]\n",
    "        )\n",
    "        for service in dialogue[\"services\"]:\n",
    "            desc = self.schemas.get_service_desc(service)\n",
    "            resp[\"value\"].append(desc)\n",
    "            resp[\"tokens\"].append(self.tokenizer(desc))\n",
    "            resp[\"ids\"].append(self.token_indexer(resp[\"tokens\"][-1]))\n",
    "            resp[\"mask\"].append([1] * len(resp[\"tokens\"][-1]))\n",
    "        return resp\n",
    "\n",
    "    def field_turn_service_exist(self, turnid, dialogue):\n",
    "        turn = dialogue[\"turns\"][turnid]\n",
    "        services = dialogue[\"services\"]\n",
    "        # order frames by dialog.services list, to establish one to one mappings across fields\n",
    "        sorted_frames = sorted(turn[\"frames\"], key=lambda x: services.index(x[\"service\"]))\n",
    "        exists_onehot = label_binarize([f[\"service\"] for f in sorted_frames], classes=services)\n",
    "        exists = np.sum(exists_onehot, axis=0) # eg: [1, 0, 1, 0]\n",
    "        return {\"ids\": exists, \"padding\": -1}\n",
    "        \n",
    "    def field_intent(self, dialogue):\n",
    "        return {\"value\": [self.schemas.get(s)[\"intent_name\"] for s in dialogue[\"services\"]]}\n",
    "\n",
    "    def field_intent_desc(self, dialogue):\n",
    "        resp = dict(\n",
    "            value=[],\n",
    "            tokens=[],\n",
    "            ids=[],\n",
    "            mask=[]\n",
    "        )\n",
    "        for service in dialogue[\"services\"]:\n",
    "            s_desc = [d for d in self.schemas.get(service)[\"intent_desc\"]]\n",
    "            s_tokens = [self.tokenizer(d) for d in s_desc]\n",
    "            s_ids = [self.token_indexer(d) for d in s_tokens]\n",
    "            s_mask = [[1] * len(d) for d in s_tokens]\n",
    "            resp[\"value\"].append(s_desc)\n",
    "            resp[\"tokens\"].append(s_tokens)\n",
    "            resp[\"ids\"].append(s_ids)\n",
    "            resp[\"mask\"].append(s_mask)\n",
    "        return resp\n",
    "\n",
    "    def field_turn_intent_exist(self, turnid, dialogue):\n",
    "        turn = dialogue[\"turns\"][turnid]\n",
    "        if turn[\"speaker\"] == \"USER\":\n",
    "            # maintain order of services; onehot per service\n",
    "            exists_onehot = OrderedDict()\n",
    "            for service in dialogue[\"services\"]:\n",
    "                exists_onehot[service] = None\n",
    "            \n",
    "            # fill encodings of existing services\n",
    "            # this _will_ be onehot assuming each service has only one intent!\n",
    "            for frame in turn[\"frames\"]:\n",
    "                service = frame[\"service\"]\n",
    "                all_intents = self.schemas.get(service)[\"intent_name\"]\n",
    "                intent = frame[\"state\"][\"active_intent\"]\n",
    "                encoding = label_binarize([intent], classes=all_intents)[0]\n",
    "                exists_onehot[service] = encoding\n",
    "            \n",
    "            # fill with empty encodings for remaining\n",
    "            for service in exists_onehot:\n",
    "                if exists_onehot[service] is None:\n",
    "                    all_intents = self.schemas.get(service)[\"intent_name\"]\n",
    "                    encoding = np.array([0] * len(all_intents))\n",
    "                    exists_onehot[service] = encoding\n",
    "\n",
    "            return {\"ids\": list(exists_onehot.values()), \"padding\": -1}\n",
    "\n",
    "    def field_turn_intent_changed(self, turnid, dialogue):\n",
    "        turn = dialogue[\"turns\"][turnid]\n",
    "        if turn[\"speaker\"] == \"USER\":\n",
    "            # assumes system turn is always followed by user turn\n",
    "            prev_user_turn = dialogue[\"turns\"][turnid-2] if turnid >= 2 else turn\n",
    "            # maintain order of services: service -> changed\n",
    "            intent_changed = OrderedDict()\n",
    "            for service in dialogue[\"services\"]:\n",
    "                intent_changed[service] = 0\n",
    "            \n",
    "            for frame, prevframe in zip(turn[\"frames\"], prev_user_turn[\"frames\"]):\n",
    "                service = frame[\"service\"]\n",
    "                intent = frame[\"state\"][\"active_intent\"]\n",
    "                prev_intent = prevframe[\"state\"][\"active_intent\"]\n",
    "                intent_changed[service] = int(intent == prev_intent)\n",
    "            \n",
    "            values = list(intent_changed.values())\n",
    "            return {\"ids\": values, \"padding\": -1}\n",
    "\n",
    "    def field_slots(self, dialogue):\n",
    "        slot_list = []\n",
    "        for service in dialogue[\"services\"]:\n",
    "            slots = self.schemas.get(service)[\"slot_name\"]\n",
    "            slot_list.append(slots)\n",
    "        return {\"value\": slot_list}\n",
    "\n",
    "    def field_slots_desc(self, dialogue):\n",
    "        resp = dict(\n",
    "            value=[],\n",
    "            tokens=[],\n",
    "            ids=[],\n",
    "            mask=[]\n",
    "        )\n",
    "        for service in dialogue[\"services\"]:\n",
    "            s_desc = [d for d in self.schemas.get(service)[\"slot_desc\"]]\n",
    "            s_tokens = [self.tokenizer(d) for d in s_desc]\n",
    "            s_ids = [self.token_indexer(d) for d in s_tokens]\n",
    "            s_mask = [[1] * len(d) for d in s_tokens]\n",
    "            resp[\"value\"].append(s_desc)\n",
    "            resp[\"tokens\"].append(s_tokens)\n",
    "            resp[\"ids\"].append(s_ids)\n",
    "            resp[\"mask\"].append(s_mask)\n",
    "        return resp\n",
    "\n",
    "    def field_slots_iscat(self, dialogue):\n",
    "        iscat_list = []\n",
    "        for service in dialogue[\"services\"]:\n",
    "            iscat = [int(i) for i in self.schemas.get(service)[\"slot_iscat\"]]\n",
    "            iscat_list.append(iscat)\n",
    "        return {\"ids\": iscat_list, \"padding\": -1}\n",
    "\n",
    "    def field_num_turns(self, dialogue):\n",
    "        return {\"value\": len(dialogue[\"turns\"])}\n",
    "\n",
    "    def field_turn_num_frames(self, turnid, dialogue):\n",
    "        return {\"value\": len(dialogue[\"turns\"][turnid][\"frames\"])}\n",
    "\n",
    "    def text_to_fields(self, dialogue):\n",
    "        \"\"\"\n",
    "        fields = dict(\n",
    "            dialogue_id=None, # [Batch,]\n",
    "            num_turns=None, # [Batch,]\n",
    "            num_frames=[], # [Batch, Turn] equals to number of services per turn\n",
    "\n",
    "            # messages\n",
    "            speaker=[], # [Batch, Turn]\n",
    "            utter=[], # [Batch, Turn, Tokens]\n",
    "            sys_utter=[], # [Batch, Turn, Tokens] only system utters\n",
    "            usr_utter=[], # [Batch, Turn, Tokens] only user utters\n",
    "\n",
    "            # services\n",
    "            service=None, # [Batch, Service] all dialog services\n",
    "            service_desc=None, # [Batch, Service, Tokens] service descriptions\n",
    "            service_exist=[], # [Batch, Turn, Service] binarized\n",
    "            \n",
    "            # intents\n",
    "            intent=None, # [Batch, Service, Intent]\n",
    "            intent_desc=None, # [Batch, Service, Intent, Tokens]\n",
    "            intent_exist=[], # [Batch, Turn, Service, Intent]\n",
    "            intent_changed=[], # [Batch, Turn, Service]\n",
    "\n",
    "            # state slots\n",
    "            slots=None, # [Batch, Service, Slot]\n",
    "            slots_desc=None, # [Batch, Service, Slot, Tokens]\n",
    "            slots_iscat=None, # [Batch, Service, Slot]\n",
    "        )\n",
    "        \"\"\"\n",
    "        fields = {}\n",
    "        \n",
    "        # filter the field names in the instance\n",
    "        dial_field_funcs = []\n",
    "        turn_field_funcs = []\n",
    "        for attr in dir(self):\n",
    "            if attr.startswith(\"field_turn_\"):\n",
    "                turn_field_funcs.append(attr)\n",
    "            elif attr.startswith(\"field_\"):\n",
    "                dial_field_funcs.append(attr)\n",
    "        \n",
    "        # fill dialogue level fields\n",
    "        for func in dial_field_funcs:\n",
    "            name = func.split(\"field_\", maxsplit=1)[-1]\n",
    "            resp = getattr(self, func)(dialogue)\n",
    "            resp[\"padding\"] = resp.get(\"padding\", self.default_padding)\n",
    "            fields[name] = resp\n",
    "\n",
    "        # fill turn level fields\n",
    "        for turnid in range(len(dialogue[\"turns\"])):\n",
    "            for func in turn_field_funcs:\n",
    "                name = attr.split(\"field_turn_\", maxsplit=1)[-1]\n",
    "                resp = getattr(self, func)(turnid, dialogue) or {}\n",
    "                if name not in fields:\n",
    "                    fields[name] = {\"padding\": resp.get(\"padding\", self.default_padding)}\n",
    "                for k, v in resp.items():\n",
    "                    if k != \"padding\":\n",
    "                        fields[name][k] = fields[name].get(k, [])\n",
    "                        fields[name][k].append(v)\n",
    "        \n",
    "        # combine the turn field ids and mask.. with default padding or the one given by func resp\n",
    "        for name, data in fields.items():\n",
    "            padding_value = data[\"padding\"]\n",
    "            for attr in [\"ids\", \"mask\"]:\n",
    "                if attr in data:\n",
    "                    data[attr] = padded_array(data[attr], padding_value)\n",
    "            \n",
    "        return fields\n",
    "    \n",
    "schemas = Schemas(\"../data/train/schema.json\")\n",
    "ds = DialogueDataset(\"../data/train/dialogues_001.json\", schemas, tokenizer, token_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dialogue_mini_batcher(dialogues):\n",
    "    batch = {}\n",
    "    for dial in dialogues:\n",
    "        # populate the batch\n",
    "        for field, data in dial.items():\n",
    "            if field not in batch:\n",
    "                batch[field] = {}\n",
    "            for attr, val in data.items():\n",
    "                if attr == \"padding\":\n",
    "                    batch[field][attr] = val\n",
    "                else:\n",
    "                    batch[field][attr] = batch[field].get(attr, [])\n",
    "                    batch[field][attr].append(val)\n",
    "\n",
    "    # padding on field attributes\n",
    "    for field_name, data in batch.items():\n",
    "        for attr in [\"ids\", \"mask\"]:\n",
    "            if attr in data:\n",
    "                data[attr] = padded_array(data[attr], data[\"padding\"])\n",
    "                data[attr] = torch.tensor(data[attr], device=\"cpu\")\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dial_sets = []\n",
    "# for filename in tqdm(glob.glob(\"../data/train/dialogues*.json\")):\n",
    "#     ds = DialogueDataset(filename, schemas, tokenizer, token_indexer)\n",
    "#     dial_sets.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc3498672b0427b8ac7896ae9490085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=127), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dial_sets = []\n",
    "dial_files = glob.glob(\"../data/train/dialogues*.json\")\n",
    "\n",
    "def create_dataset(filename):\n",
    "    return DialogueDataset(filename, schemas, tokenizer, token_indexer)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    for ds in tqdm(executor.map(create_dataset, dial_files), total=len(dial_files)):\n",
    "        dial_sets.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = D.ConcatDataset(dial_sets)\n",
    "torch.save(train_ds, \"../data/preprocessed/train_ds2.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "190px",
    "left": "1950px",
    "right": "20px",
    "top": "120px",
    "width": "295px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as D\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import re\n",
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "from copy import deepcopy\n",
    "from functools import lru_cache\n",
    "from collections import OrderedDict\n",
    "from types import SimpleNamespace\n",
    "from collections.abc import Iterable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from ipdb import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        self.bert = bert\n",
    "    \n",
    "    def __call__(self, text, include_sep=True):\n",
    "        tokens = self.bert.tokenize(text)\n",
    "        if include_sep:\n",
    "            tokens.insert(0, \"[CLS]\")\n",
    "            tokens.append(\"[SEP]\")\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "class TokenIndexer:\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        self.bert = bert\n",
    "        \n",
    "    def inv(self, *args, **kw):\n",
    "        # tokens MUST be list, tensor doesn't work.\n",
    "        return self.bert.convert_ids_to_tokens(*args, **kw)\n",
    "        \n",
    "    def __call__(self, *args, **kw):\n",
    "        return self.bert.convert_tokens_to_ids(*args, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def label_binarize(labels, classes):\n",
    "    # labels: np.array or tensor [batch, 1]\n",
    "    # classes: [..] list of classes\n",
    "    # weirdly,`sklearn.preprocessing.label_binarize` returns [1] or [0]\n",
    "    # instead of onehot ONLY when executing in this script!\n",
    "    vectors = [np.zeros(len(classes)) for _ in labels]\n",
    "    for i, label in enumerate(labels):\n",
    "        for j, c in enumerate(classes):\n",
    "            if c == label:\n",
    "                vectors[i][j] = 1\n",
    "    return np.array(vectors)\n",
    "    \n",
    "\n",
    "def label_inv_binarize(vectors, classes):\n",
    "    # labels: np.array or tensor [batch, classes]\n",
    "    # classes: [..] list of classes\n",
    "    # follows sklearn LabelBinarizer.inverse_transform()\n",
    "    # given all zeros, predicts label at index 0, instead of returning none!\n",
    "    # sklearn doesn't have functional API of inverse transform\n",
    "    labels = []\n",
    "    for each in vectors:\n",
    "        index = np.argmax(each)\n",
    "        labels.append(classes[index])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def padded_array(array, value=0):\n",
    "    # TODO: this does not do type checking; and wow it can be slow on strings.\n",
    "    # expects array to have fixed _number_ of dimensions\n",
    "    \n",
    "    # resolve the shape of padded array\n",
    "    shape_index = {}\n",
    "    queue = [(array, 0)]\n",
    "    while queue:\n",
    "        subarr, dim = queue.pop(0)\n",
    "        shape_index[dim] = max(shape_index.get(dim, -1), len(subarr))\n",
    "        for x in subarr:\n",
    "            if isinstance(x, Iterable) and not isinstance(x, str):\n",
    "                queue.append((x, dim+1))\n",
    "    shape = [shape_index[k] for k in range(max(shape_index) + 1)]\n",
    "    \n",
    "    # fill the values \n",
    "    padded = np.ones(shape) * value\n",
    "    queue = [(array, [])]\n",
    "    while queue:\n",
    "        subarr, index = queue.pop(0)\n",
    "        for j, x in enumerate(subarr):\n",
    "            if isinstance(x, Iterable):\n",
    "                queue.append((x, index + [j]))\n",
    "            else:\n",
    "                padded[tuple(index + [j])] = x\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Schemas(object):\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath) as f:\n",
    "            self.index = {}\n",
    "            for schema in json.load(f):\n",
    "                service_name = schema[\"service_name\"]\n",
    "                self.index[service_name] = schema\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get(self, service):\n",
    "        result = dict(\n",
    "            # service\n",
    "            name=service,\n",
    "            desc=self.index[service][\"description\"],\n",
    "            \n",
    "            # slots\n",
    "            slot_name=[],\n",
    "            slot_desc=[],\n",
    "            slot_iscat=[], \n",
    "            slot_vals=[], # collected only for cat slots.. not sure if that makes sense\n",
    "\n",
    "            # intents\n",
    "            intent_name=[],\n",
    "            intent_desc=[],\n",
    "            intent_istrans=[],\n",
    "            intent_reqslots=[],\n",
    "            intent_optslots=[],\n",
    "            intent_optvals=[],\n",
    "        )\n",
    "\n",
    "        for slot in self.index[service][\"slots\"]:\n",
    "            result[\"slot_name\"].append(slot[\"name\"])\n",
    "            result[\"slot_desc\"].append(slot[\"description\"])\n",
    "            result[\"slot_iscat\"].append(slot[\"is_categorical\"])\n",
    "            result[\"slot_vals\"].append(slot[\"possible_values\"])\n",
    "        \n",
    "        for intent in self.index[service][\"intents\"]:\n",
    "            result[\"intent_name\"].append(intent[\"name\"])\n",
    "            result[\"intent_desc\"].append(intent[\"description\"])\n",
    "            result[\"intent_istrans\"].append(intent[\"is_transactional\"])\n",
    "            result[\"intent_reqslots\"].append(intent[\"required_slots\"])\n",
    "            result[\"intent_optslots\"].append(list(intent[\"optional_slots\"].keys()))\n",
    "            result[\"intent_optvals\"].append(list(intent[\"optional_slots\"].values()))\n",
    "\n",
    "        return result    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialogue Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueDataset(D.Dataset):\n",
    "    def __init__(self, filename, schemas, tokenizer, token_indexer):\n",
    "        with open(filename) as f:\n",
    "            self.ds = json.load(f)\n",
    "        self.schemas = schemas\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexer = token_indexer\n",
    "        self.dialogues = []\n",
    "        for dial in self.ds:\n",
    "            fields = self.dial_to_fields(dial)\n",
    "            self.dialogues.append(fields)\n",
    "        self.schemas = None\n",
    "        self.tokenizer = None\n",
    "        self.token_indexer = None\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dialogues[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "    \n",
    "    def fd_serv_name(self, dial, fields):\n",
    "        resp = dict(value=[])\n",
    "        for service in dial[\"services\"]:\n",
    "            resp[\"value\"].append(service)\n",
    "        return resp\n",
    "        \n",
    "    def fd_serv_desc(self, dial, fields):\n",
    "        resp = dict(value=[], tokens=[], ids=[], mask=[])\n",
    "        for service in dial[\"services\"]:\n",
    "            desc = self.schemas.get(service)[\"desc\"]\n",
    "            resp[\"value\"].append(desc)\n",
    "            resp[\"tokens\"].append(self.tokenizer(desc))\n",
    "            resp[\"ids\"].append(self.token_indexer(resp[\"tokens\"][-1]))\n",
    "            resp[\"mask\"].append([1] * len(resp[\"tokens\"][-1]))\n",
    "        return resp\n",
    "    \n",
    "    def fd_slot_name(self, dial, fields):\n",
    "        resp = {\"value\": []}\n",
    "        for serv in dial[\"services\"]:\n",
    "            schema = self.schemas.get(serv)\n",
    "            resp[\"value\"].append(schema[\"slot_name\"])\n",
    "        return resp\n",
    "    \n",
    "    def fd_slot_desc(self, dial, fields):\n",
    "        resp = dict(value=[], tokens=[], ids=[], mask=[])\n",
    "        for serv in dial[\"services\"]:\n",
    "            s_desc = self.schemas.get(serv)[\"slot_desc\"]\n",
    "            s_tokens = [self.tokenizer(d) for d in s_desc]\n",
    "            s_ids = [self.token_indexer(d) for d in s_tokens]\n",
    "            s_mask = [[1] * len(d) for d in s_tokens]\n",
    "            resp[\"value\"].append(s_desc)\n",
    "            resp[\"tokens\"].append(s_tokens)\n",
    "            resp[\"ids\"].append(s_ids)\n",
    "            resp[\"mask\"].append(s_mask)\n",
    "        return resp\n",
    "\n",
    "    def fd_slot_memory(self, dial, fields):\n",
    "        # memory is a sequence of slot tagger values across all frames in a dial.. that grows per turn\n",
    "        # maintain snapshot at each turn\n",
    "        resp = dict(\n",
    "            value=[], # unfeaturized memory\n",
    "            tokens=[], # tokenized memory [turn, mem-size, tokens]\n",
    "            ids=[], # indexed memory [turn mem-size, tokens]\n",
    "            mask=[], # mask on memory [turn, mem-size, tokens]\n",
    "            ids_memsize=[] # mem sizes [turn, 1]\n",
    "        )\n",
    "        \n",
    "        # memory is sequential, initialized by vals from schema, only has values\n",
    "        memory = [\"NONE\", \"dontcare\"] # keep these at index 0, 1\n",
    "        memory_index = set()\n",
    "        for serv in dial[\"services\"]:\n",
    "            schema = self.schemas.get(serv)\n",
    "            for values in schema[\"slot_vals\"]:\n",
    "                for val in values:\n",
    "                    if val not in memory_index:\n",
    "                        memory.append(val)\n",
    "                        memory_index.add(val)\n",
    "        \n",
    "        # at each user turn create a memory snapshot..\n",
    "        for turn in dial[\"turns\"]:\n",
    "            memory = deepcopy(memory)\n",
    "            # pick slot tagger's ground truth ; categorical slot vals are already initialized!\n",
    "            utter = turn[\"utterance\"]\n",
    "            for frame in turn[\"frames\"]:\n",
    "                for tag in frame[\"slots\"]:\n",
    "                    st, en = tag[\"start\"], tag[\"exclusive_end\"]\n",
    "                    value = utter[st:en]\n",
    "                    if value not in memory_index:\n",
    "                        memory.append(value)\n",
    "                        memory_index.add(value)\n",
    "            if turn[\"speaker\"] == \"USER\":\n",
    "                resp[\"value\"].append(memory)\n",
    "                resp[\"ids_memsize\"].append(len(memory))\n",
    "\n",
    "        # tokenize and index the memory values\n",
    "        # value: [turn, values], ids/tokens: [turn, values, tokens]\n",
    "        for mem_snapshot in resp[\"value\"]:\n",
    "            mem_tokens = []\n",
    "            mem_ids = []\n",
    "            mem_mask = []\n",
    "            for val in mem_snapshot:\n",
    "                tokens = self.tokenizer(val)\n",
    "                mem_tokens.append(tokens)\n",
    "                mem_ids.append(self.token_indexer(tokens))\n",
    "                mem_mask.append([1] * len(tokens))\n",
    "            resp[\"tokens\"].append(mem_tokens)\n",
    "            resp[\"ids\"].append(mem_ids)\n",
    "            resp[\"mask\"].append(mem_mask)\n",
    "        \n",
    "        return resp\n",
    "    \n",
    "    def fd_slot_memory_loc(self, dial, fields):\n",
    "        resp = dict(\n",
    "            value=[],  # string value\n",
    "            ids=[], # memory loc\n",
    "            ids_onehot=[], # onehot of memory loc\n",
    "            mask=[],\n",
    "            mask_onehot=[],\n",
    "            mask_none=[], # 1 -> non NONE values\n",
    "            mask_none_onehot=[],\n",
    "        )\n",
    "        \n",
    "        # query dialog memory snapshots per turn. NOTE: fd_slot_memory should exec first.\n",
    "        memory = fields[\"slot_memory\"][\"value\"]\n",
    "        \n",
    "        # init snapshot: service, slot -> memory loc, val\n",
    "        memory_loc = []\n",
    "        memory_val = []\n",
    "        for turn in dial[\"turns\"]:\n",
    "            if turn[\"speaker\"] == \"USER\":\n",
    "                loc = OrderedDict()\n",
    "                val = OrderedDict()\n",
    "                for serv in dial[\"services\"]:\n",
    "                    loc[serv] = OrderedDict()\n",
    "                    val[serv] = OrderedDict()\n",
    "                    for slot in self.schemas.get(serv)[\"slot_name\"]:\n",
    "                        loc[serv][slot] = None\n",
    "                        val[serv][slot] = None\n",
    "                memory_loc.append(loc)\n",
    "                memory_val.append(val)\n",
    "        \n",
    "        # fill the memory locations \n",
    "        snapshot_id = 0\n",
    "        for turn in dial[\"turns\"]:\n",
    "            if turn[\"speaker\"] == \"USER\":\n",
    "                turn_memory = memory[snapshot_id]\n",
    "                turn_memory_loc = memory_loc[snapshot_id]\n",
    "                turn_memory_val = memory_val[snapshot_id]\n",
    "                for frame in turn[\"frames\"]:\n",
    "                    service = frame[\"service\"]\n",
    "                    for slot, values in frame[\"state\"][\"slot_values\"].items():\n",
    "                        val = re.sub(\"\\u2013\", \"-\", values[0]) # dial 59_00125 turn 14\n",
    "                        turn_memory_loc[service][slot] = turn_memory.index(val)\n",
    "                        turn_memory_val[service][slot] = val\n",
    "                    # add locations to UNKNOWN slots\n",
    "                    for slot, val in turn_memory_loc[service].items():\n",
    "                        if val is None:\n",
    "                            turn_memory_loc[service][slot] = turn_memory.index(\"NONE\")\n",
    "                            turn_memory_val[service][slot] = \"NONE\"\n",
    "                snapshot_id += 1\n",
    "        \n",
    "        # featurize\n",
    "        snapshot_id = 0\n",
    "        for turn in dial[\"turns\"]:\n",
    "            if turn[\"speaker\"] == \"USER\":\n",
    "                turn_memory = memory[snapshot_id]\n",
    "                turn_memory_loc = memory_loc[snapshot_id]\n",
    "                turn_memory_val = memory_val[snapshot_id]\n",
    "                none_loc = memory[snapshot_id].index(\"NONE\")\n",
    "                turn_fields = dict(\n",
    "                    value=[], ids=[], ids_onehot=[], mask=[], mask_onehot=[],\n",
    "                    mask_none=[], mask_none_onehot=[],\n",
    "                )\n",
    "\n",
    "                for serv in turn_memory_loc:\n",
    "                    mem_size = len(turn_memory)\n",
    "                    vals = list(turn_memory_val[serv].values())\n",
    "                    ids = list(turn_memory_loc[serv].values())\n",
    "                    ids_onehots = label_binarize(ids, list(range(mem_size)))\n",
    "                    mask = [1] * len(ids)\n",
    "                    mask_onehots = [[1] * mem_size for _ in ids]\n",
    "\n",
    "                    mask_none = [int(v!=\"NONE\") for v in vals]\n",
    "                    mask_none_onehot = [[1] * mem_size for _ in ids]\n",
    "                    for v, loc, onehot in zip(vals, ids, mask_none_onehot):\n",
    "                        if v == \"NONE\":\n",
    "                            onehot[loc] = 0\n",
    "                                \n",
    "                    turn_fields[\"value\"].append(vals)\n",
    "                    turn_fields[\"ids\"].append(ids)\n",
    "                    turn_fields[\"ids_onehot\"].append(ids_onehots)\n",
    "                    turn_fields[\"mask\"].append(mask)\n",
    "                    turn_fields[\"mask_onehot\"].append(mask_onehots)\n",
    "                    turn_fields[\"mask_none\"].append(mask_none)\n",
    "                    turn_fields[\"mask_none_onehot\"].append(mask_none_onehot)\n",
    "\n",
    "                # update turn\n",
    "                for k, v in turn_fields.items():\n",
    "                    resp[k].append(v)\n",
    "                \n",
    "                snapshot_id += 1\n",
    "            \n",
    "        return resp\n",
    "    \n",
    "    def fd_num_turns(self, dial, fields):\n",
    "        return {\"ids\": len(dial[\"turns\"])}\n",
    "    \n",
    "    def fd_num_frames(self, dial, fields):\n",
    "        return {\"ids\": [len(t[\"frames\"]) for t in dial[\"turns\"]]}\n",
    "    \n",
    "    def fd_usr_utter(self, dial, fields):\n",
    "        resp = dict(value=[], ids=[], mask=[], tokens=[])\n",
    "        for turn in dial[\"turns\"]:\n",
    "            if turn[\"speaker\"] == \"USER\":\n",
    "                utter = turn[\"utterance\"]\n",
    "                tokens = self.tokenizer(utter)\n",
    "                ids = self.token_indexer(tokens)\n",
    "                resp[\"value\"].append(utter)\n",
    "                resp[\"ids\"].append(ids)\n",
    "                resp[\"tokens\"].append(tokens)\n",
    "                resp[\"mask\"].append([1] * len(tokens))\n",
    "        return resp\n",
    "    \n",
    "    def fd_sys_utter(self, dial, fields):\n",
    "        resp = dict(value=[], ids=[], mask=[], tokens=[])\n",
    "        for turn in dial[\"turns\"]:\n",
    "            if turn[\"speaker\"] == \"SYSTEM\":\n",
    "                utter = turn[\"utterance\"]\n",
    "                tokens = self.tokenizer(utter)\n",
    "                ids = self.token_indexer(tokens)\n",
    "                resp[\"value\"].append(utter)\n",
    "                resp[\"ids\"].append(ids)\n",
    "                resp[\"tokens\"].append(tokens)\n",
    "                resp[\"mask\"].append([1] * len(tokens))\n",
    "        return resp\n",
    "    \n",
    "    def fd_dial_id(self, dial, fields):\n",
    "        return {\"value\": dial[\"dialogue_id\"]}\n",
    "    \n",
    "    def dial_to_fields(self, dial):\n",
    "        fields = {}\n",
    "        ordered_funcs = [\n",
    "            \"fd_dial_id\",\n",
    "            \"fd_num_turns\", \"fd_num_frames\",\n",
    "            \"fd_serv_name\", \"fd_serv_desc\", \n",
    "            \"fd_slot_name\", \"fd_slot_desc\",\n",
    "            \"fd_slot_memory\", \"fd_slot_memory_loc\",\n",
    "            \"fd_usr_utter\", \"fd_sys_utter\"]\n",
    "        for func in ordered_funcs:\n",
    "            name = func.split(\"fd_\", maxsplit=1)[-1]\n",
    "            value = getattr(self, func)(dial, fields)\n",
    "            if value is not None:\n",
    "                fields[name] = value\n",
    "        return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "bert_ = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = Tokenizer(bert_)\n",
    "token_indexer = TokenIndexer(bert_)\n",
    "\n",
    "train_schemas = Schemas(\"../data/train/schema.json\")\n",
    "test_schemas = Schemas(\"../data/dev/schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87108653ff448d4b60c2bfc8e7858b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=127), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "train_dial_sets = []\n",
    "train_dial_files = sorted(glob.glob(\"../data/train/dialogues*.json\"))\n",
    "num_workers = min(20, len(train_dial_files))\n",
    "\n",
    "def worker(filename):\n",
    "    return DialogueDataset(filename, train_schemas, tokenizer, token_indexer)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "    for ds in tqdm(executor.map(worker, train_dial_files), total=len(train_dial_files)):\n",
    "        train_dial_sets.append(ds)\n",
    "\n",
    "train_ds = D.ConcatDataset(train_dial_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07802615398543608d1eeee6a3027801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load test dataset\n",
    "test_dial_sets = []\n",
    "test_dial_files = sorted(glob.glob(\"../data/dev/dialogues*.json\"))\n",
    "num_workers = min(20, len(train_dial_files))\n",
    "\n",
    "def worker(filename):\n",
    "    return DialogueDataset(filename, test_schemas, tokenizer, token_indexer)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "    for ds in tqdm(executor.map(worker, test_dial_files), total=len(test_dial_files)):\n",
    "        test_dial_sets.append(ds)\n",
    "\n",
    "test_ds = D.ConcatDataset(test_dial_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def move_to_device(obj, device):\n",
    "    if type(obj) is list:\n",
    "        return [move_to_device(o, device) for o in obj]\n",
    "    elif type(obj) is dict:\n",
    "        return {k: move_to_device(v, device) for k, v in obj.items()}\n",
    "    elif type(obj) is torch.Tensor or isinstance(obj, nn.Module):\n",
    "        return obj.to(device)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dialogue_mini_batcher(dialogues):\n",
    "    default_padding = 0\n",
    "    batch = {}\n",
    "    for dial in dialogues:\n",
    "        # populate the batch\n",
    "        for field, data in dial.items():\n",
    "            if field not in batch:\n",
    "                batch[field] = {}\n",
    "            for attr, val in data.items():\n",
    "                if attr == \"padding\":\n",
    "                    batch[field][attr] = val\n",
    "                else:\n",
    "                    batch[field][attr] = batch[field].get(attr, [])\n",
    "                    batch[field][attr].append(val)\n",
    "\n",
    "    # padding on field attributes\n",
    "    for field_name, data in batch.items():\n",
    "        for attr in data:\n",
    "            if attr.startswith(\"ids\") or attr.startswith(\"mask\"):\n",
    "                if type(data[attr]) is not torch.Tensor: # don't move b/w CPU and GPU unnecessarily\n",
    "                    data[attr] = padded_array(data[attr], default_padding)\n",
    "                    data[attr] = torch.tensor(data[attr], device=\"cpu\")\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "class DialogIterator(object):\n",
    "    \"\"\"A simple wrapper on DataLoader\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size, *args, **kw):\n",
    "        self.length = None\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.iterator = D.DataLoader(dataset, batch_size, *args, **kw)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.length is None:\n",
    "            self.length = 0\n",
    "            for dial in self.dataset:\n",
    "                self.length += dial[\"num_turns\"][\"ids\"] + len(dial[\"serv_name\"][\"value\"])\n",
    "            self.length = int(self.length / self.batch_size)\n",
    "        return self.length\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.iterator:\n",
    "            num_turns = batch[\"usr_utter\"][\"ids\"].shape[1]\n",
    "            num_services = batch[\"serv_desc\"][\"ids\"].shape[1]\n",
    "            for turnid in range(num_turns):\n",
    "                for sid in range(num_services):\n",
    "                    inputs = dict(turnid=turnid, serviceid=sid)\n",
    "                    inputs.update(batch)\n",
    "                    yield inputs\n",
    "                    \n",
    "#next(iter(DialogIterator(train_ds, 1, collate_fn=dialogue_mini_batcher)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_turns ids --> torch.Size([1])\n",
      "num_frames ids --> torch.Size([1, 24])\n",
      "serv_desc ids --> torch.Size([1, 1, 10])\n",
      "serv_desc mask --> torch.Size([1, 1, 10])\n",
      "slot_desc ids --> torch.Size([1, 1, 11, 12])\n",
      "slot_desc mask --> torch.Size([1, 1, 11, 12])\n",
      "slot_memory ids --> torch.Size([1, 12, 29, 10])\n",
      "slot_memory mask --> torch.Size([1, 12, 29, 10])\n",
      "slot_memory ids_memsize --> torch.Size([1, 12])\n",
      "slot_memory_loc ids --> torch.Size([1, 12, 1, 11])\n",
      "slot_memory_loc ids_onehot --> torch.Size([1, 12, 1, 11, 29])\n",
      "slot_memory_loc mask --> torch.Size([1, 12, 1, 11])\n",
      "slot_memory_loc mask_onehot --> torch.Size([1, 12, 1, 11, 29])\n",
      "slot_memory_loc mask_none --> torch.Size([1, 12, 1, 11])\n",
      "slot_memory_loc mask_none_onehot --> torch.Size([1, 12, 1, 11, 29])\n",
      "usr_utter ids --> torch.Size([1, 12, 25])\n",
      "usr_utter mask --> torch.Size([1, 12, 25])\n",
      "sys_utter ids --> torch.Size([1, 12, 31])\n",
      "sys_utter mask --> torch.Size([1, 12, 31])\n"
     ]
    }
   ],
   "source": [
    "def print_shapes(ds):\n",
    "    it = next(iter(DialogIterator(ds, 1, collate_fn=dialogue_mini_batcher)))\n",
    "    for field, val in it.items():\n",
    "        if type(val) is dict:\n",
    "            for attr in val:\n",
    "                if attr.startswith(\"ids\") or attr.startswith(\"mask\"):\n",
    "                    print(field, attr, \"-->\", val[attr].shape)\n",
    "                    \n",
    "\n",
    "# TODO: why mem ids, and mem loc size is diff by 1?\n",
    "print_shapes(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import BertModel, BertConfig\n",
    "from allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDownstream(nn.Module):\n",
    "    # https://huggingface.co/pytorch-transformers/model_doc/bert.html\n",
    "    \n",
    "    def __init__(self, btype, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.emb = BertModel.from_pretrained(btype)\n",
    "        for param in self.emb.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "    \n",
    "    def forward(self, *args, **kw):\n",
    "        outputs = self.emb(*args, **kw)\n",
    "        return outputs[0] # pooler [b,s]->[b,s,e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateSelector(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = BertDownstream(\"bert-base-uncased\")\n",
    "        self.emb_dim = 768\n",
    "        \n",
    "        # encode utter and desc tokens\n",
    "        self.l0 = nn.GRU(self.emb_dim, self.emb_dim, batch_first=True, num_layers=1)\n",
    "        \n",
    "        # encode memory cells and rank \n",
    "        self.l1 = nn.GRU(self.emb_dim, self.emb_dim, batch_first=True, num_layers=1)\n",
    "\n",
    "        # encode slot desc\n",
    "        self.l2 = nn.GRU(self.emb_dim, self.emb_dim, batch_first=True, num_layers=1)\n",
    "       \n",
    "        # attentions\n",
    "        self.l4 = nn.Linear(2*self.emb_dim, self.emb_dim)\n",
    "        self.l6 = nn.Bilinear(self.emb_dim, self.emb_dim, 1, bias=False)\n",
    "         \n",
    "        # metrics\n",
    "        self.acc = CategoricalAccuracy()\n",
    "        self.goal_acc = CategoricalAccuracy()\n",
    "\n",
    "        # init weights: classifier's performance changes heavily on these\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.startswith((\"l0.\", \"11.\", \"l2.\")):\n",
    "                print(\"Initializing bias/weights of \", name)\n",
    "                if \"weight\" in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    param.data.fill_(0.)\n",
    "\n",
    "    def get_metrics(self, reset=False, goal_reset=False):\n",
    "        return dict(\n",
    "            acc=self.acc.get_metric(reset), # avg per service\n",
    "            goal_acc=self.goal_acc.get_metric(goal_reset), # avg per turn\n",
    "        )\n",
    "    \n",
    "    def compute_score1(self, memory, slot, utter):\n",
    "        # memory: bme\n",
    "        # slot: bse\n",
    "        # utter: b,e*dir\n",
    "        s = slot.shape[1]\n",
    "        m = memory.shape[1]\n",
    "        \n",
    "        utter = utter[:,None,:].expand(-1,s,-1) #be->bse\n",
    "\n",
    "        key = self.l4(torch.cat([utter, slot], dim=-1)) #bs2e\n",
    "        key = torch.tanh(key)\n",
    "        key = key[:,:,None,:].expand(-1,-1,m,-1).contiguous() # bsme\n",
    "        \n",
    "        memory = memory[:,None,:,:].expand(-1,s,-1,-1).contiguous()\n",
    "        \n",
    "        energy = self.l6(key, memory) #bsme->bsm1\n",
    "        energy = F.softmax(energy.squeeze(-1), dim=-1)\n",
    "        return energy\n",
    "    \n",
    "    def encode_utter(self, batch):\n",
    "        turnid = batch[\"turnid\"]\n",
    "        serviceid = batch[\"serviceid\"]\n",
    "        \n",
    "        usr_utter, usr_mask = batch[\"usr_utter\"][\"ids\"][:,turnid,:], batch[\"usr_utter\"][\"mask\"][:,turnid,:]\n",
    "        sys_utter, sys_mask = batch[\"sys_utter\"][\"ids\"][:,turnid,:], batch[\"sys_utter\"][\"mask\"][:,turnid,:]\n",
    "        \n",
    "        utter = torch.cat([sys_utter, usr_utter], dim=-1).long()\n",
    "        utter_mask = torch.cat([sys_mask, usr_mask], dim=-1).long()\n",
    "        \n",
    "        utter = self.emb(utter, attention_mask=utter_mask) # bs->bse\n",
    "        utter, utter_h = self.l0(utter)\n",
    "        \n",
    "        return utter_h[-1] # be\n",
    "\n",
    "    def encode_memory(self, batch):\n",
    "        turnid = batch[\"turnid\"]\n",
    "        serviceid = batch[\"serviceid\"]\n",
    "        \n",
    "        # get fixed size encoding of memory cells\n",
    "        memory = batch[\"slot_memory\"][\"ids\"][:,turnid,:]\n",
    "        memory_mask = batch[\"slot_memory\"][\"mask\"][:,turnid,:]\n",
    "        sh = memory.shape\n",
    "        \n",
    "        # EMB can conditionally represent across memories!\n",
    "        memory = memory.contiguous().view(sh[0], -1).long() # b,m,s -> b,m*s\n",
    "        memory_mask = memory_mask.view(sh[0], -1).contiguous().long()\n",
    "        \n",
    "        memory = self.emb(memory, attention_mask=memory_mask) # b,ms -> b,ms,e\n",
    "        memory, memory_h = self.l1(memory) # b,ms,e\n",
    "        memory = memory.view(sh[0], sh[1], sh[2], -1)\n",
    "        memory = memory[:,:,-1,:]\n",
    "        \n",
    "        return memory # bme\n",
    "    \n",
    "    def encode_slot_desc(self, batch):\n",
    "        serviceid = batch[\"serviceid\"]\n",
    "        \n",
    "        desc = batch[\"slot_desc\"][\"ids\"][:,serviceid,:].long() # [batch, slots, tokens]\n",
    "        desc_mask = batch[\"slot_desc\"][\"mask\"][:,serviceid,:].long()\n",
    "        sh = desc.shape\n",
    "        \n",
    "        # bst -> b,st -> b,st,e\n",
    "        desc = desc.contiguous().view(sh[0], -1)\n",
    "        desc_mask = desc_mask.contiguous().view(sh[0], -1)\n",
    "        desc = self.emb(desc, attention_mask=desc_mask)\n",
    "        desc, desc_h = self.l2(desc)\n",
    "        desc = desc.view(sh[0], sh[1], sh[2], -1)\n",
    "        desc = desc[:,:,-1,:]\n",
    "        return desc # bse\n",
    "        \n",
    "        \n",
    "    def forward(self, **batch):\n",
    "        turnid = batch[\"turnid\"]\n",
    "        serviceid = batch[\"serviceid\"]\n",
    "        \n",
    "        # doc: GRU outputs [batch, seq, emb * dir] [layers * dir, batch, emb]\n",
    "\n",
    "        # get fixed size encodings\n",
    "        utter = self.encode_utter(batch) # be\n",
    "        memory = self.encode_memory(batch) # bme\n",
    "        slot_desc = self.encode_slot_desc(batch) #bse\n",
    "        \n",
    "        # map: slot desc -> memory\n",
    "        score = self.compute_score1(memory, slot_desc, utter) # bsm\n",
    "        output = {\"score\": score}\n",
    "\n",
    "        if \"slot_memory_loc\" in batch:\n",
    "            # calc loss\n",
    "            target_score = batch[\"slot_memory_loc\"][\"ids_onehot\"][:,turnid,serviceid,:].contiguous().view(-1, 1).float() # [batch, slots, memory]\n",
    "            target_mask = batch[\"slot_memory_loc\"][\"mask_onehot\"][:,turnid,serviceid,:].contiguous().view(-1, 1).float()\n",
    "            output[\"loss\"] = F.binary_cross_entropy(score.contiguous().view(-1,1), target_score, target_mask)\n",
    "            output[\"loss\"] = output[\"loss\"].unsqueeze(0) # don't return scalar.\n",
    "            \n",
    "            # calc acc. don't calc acc on slots that are predicted NONE.\n",
    "            target_score = batch[\"slot_memory_loc\"][\"ids\"][:,turnid,serviceid,:].float() # [batch, slots]\n",
    "            target_mask = (batch[\"slot_memory_loc\"][\"mask\"][:,turnid,serviceid,:] * \\\n",
    "                           batch[\"slot_memory_loc\"][\"mask_none\"][:,turnid,serviceid,:]).float()\n",
    "            \n",
    "            self.acc(score, target_score, target_mask)\n",
    "            self.goal_acc(score, target_score, target_mask)\n",
    "            \n",
    "            output[\"target\"] = target_score\n",
    "            output[\"target_ids\"] = torch.argmax(target_score, dim=-1)\n",
    "            output[\"pred_ids\"] = torch.argmax(score, dim=-1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.tensorboard_writer import TensorboardWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def module(m):\n",
    "    if type(m) is nn.DataParallel:\n",
    "        return m.module\n",
    "    return m\n",
    "\n",
    "def train(model, optimizer, batch_size, num_epochs, train_ds, test_ds, device):\n",
    "    current_batch = 1 # tensorboard doesn't like 0\n",
    "    tensorboard = TensorboardWriter(\n",
    "        get_batch_num_total=lambda: current_batch,\n",
    "        summary_interval=10,\n",
    "        serialization_dir=\"../data/tensorboard/\"\n",
    "    )\n",
    "    for epoch in range(num_epochs):\n",
    "        train_iter = DialogIterator(train_ds, batch_size, collate_fn=dialogue_mini_batcher)\n",
    "        num_batches = len(train_iter) \n",
    "        if test_ds:\n",
    "            test_iter = DialogIterator(test_ds, batch_size, collate_fn=dialogue_mini_batcher)\n",
    "            num_batches += len(test_iter)\n",
    "        \n",
    "        with tqdm(total=num_batches) as pbar:\n",
    "            # train\n",
    "            pbar.set_description(\"Train {}\".format(epoch))\n",
    "            model = model.train()\n",
    "            train_iter = DialogIterator(train_ds, batch_size, collate_fn=dialogue_mini_batcher)\n",
    "            metrics = OrderedDict()\n",
    "            \n",
    "            # to know when the dialog and service changes\n",
    "            turnid = -1\n",
    "            \n",
    "            for i, batch in enumerate(train_iter):\n",
    "                current_batch += 1\n",
    "                batch = move_to_device(batch, device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(**batch)\n",
    "                output[\"loss\"] = output[\"loss\"].mean()\n",
    "                output[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # at each new turn\n",
    "                if turnid != batch[\"turnid\"]:\n",
    "                    metrics.update(module(model).get_metrics(reset=True, goal_reset=True))\n",
    "                else:\n",
    "                    curr_met = module(model).get_metrics(reset=True)\n",
    "                    metrics.update(acc=curr_met[\"acc\"])\n",
    "                    \n",
    "                metrics[\"loss\"] = output[\"loss\"].item()\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(metrics)\n",
    "                \n",
    "                metrics[\"turnid\"] = batch[\"turnid\"]\n",
    "                metrics[\"servid\"] = batch[\"serviceid\"]\n",
    "                turnid = batch[\"turnid\"]\n",
    "\n",
    "#                 # update tensorboard logs\n",
    "#                 tensorboard.add_train_histogram(\"target\", output[\"target\"])\n",
    "#                 tensorboard.add_train_histogram(\"pred\", output[\"score\"])\n",
    "#                 tensorboard.add_train_histogram(\"target_ids\", output[\"target_ids\"])\n",
    "#                 tensorboard.add_train_histogram(\"pred_ids\", output[\"pred_ids\"])\n",
    "                tensorboard.add_train_scalar(\"loss\", metrics[\"loss\"], timestep=current_batch)\n",
    "                tensorboard.log_metrics(train_metrics=metrics, epoch=current_batch)\n",
    "                tensorboard.log_parameter_and_gradient_statistics(model, None)\n",
    "\n",
    "            # test\n",
    "            if test_ds:\n",
    "                pbar.set_description(\"Test {}\".format(epoch))\n",
    "                metrics = OrderedDict(epoch=epoch)\n",
    "                turnid = -1\n",
    "                with torch.no_grad():\n",
    "                    model = model.eval()\n",
    "                    for i, batch in enumerate(test_iter):\n",
    "                        current_batch += 1\n",
    "                        batch = move_to_device(batch, device)\n",
    "                        output = model(**batch)\n",
    "                        output[\"loss\"] = output[\"loss\"].mean()\n",
    "\n",
    "                        # at each new turn\n",
    "                        if turnid != batch[\"turnid\"]:\n",
    "                            metrics.update(module(model).get_metrics(reset=True, goal_reset=True))\n",
    "                        else:\n",
    "                            curr_met = module(model).get_metrics(reset=True)\n",
    "                            metrics.update(acc=curr_met[\"acc\"])\n",
    "\n",
    "                        metrics[\"loss\"] = output[\"loss\"].item()\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix(metrics)\n",
    "                        turnid = batch[\"turnid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, \"../data/model0.pkl\")\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove tensorboard logs\n",
      "set number of devices\n",
      "loading model\n",
      "Initializing bias/weights of  l0.weight_ih_l0\n",
      "Initializing bias/weights of  l0.weight_hh_l0\n",
      "Initializing bias/weights of  l0.bias_ih_l0\n",
      "Initializing bias/weights of  l0.bias_hh_l0\n",
      "Initializing bias/weights of  l2.weight_ih_l0\n",
      "Initializing bias/weights of  l2.weight_hh_l0\n",
      "Initializing bias/weights of  l2.bias_ih_l0\n",
      "Initializing bias/weights of  l2.bias_hh_l0\n",
      "started training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2621082df72e4d43babc55a36af7587c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=545), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-112-8768bb56e119>\u001b[0m(108)\u001b[0;36mencode_slot_desc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    107 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 108 \u001b[0;31m        \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    109 \u001b[0;31m        \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m> \u001b[0;32m<ipython-input-112-8768bb56e119>\u001b[0m(108)\u001b[0;36mencode_slot_desc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    107 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 108 \u001b[0;31m        \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    109 \u001b[0;31m        \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\n",
      "> \u001b[0;32m<ipython-input-112-8768bb56e119>\u001b[0m(108)\u001b[0;36mencode_slot_desc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    107 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 108 \u001b[0;31m        \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    109 \u001b[0;31m        \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> desc.shape\n",
      "torch.Size([10, 132, 768])\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-112-8768bb56e119>\u001b[0m(109)\u001b[0;36mencode_slot_desc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    108 \u001b[0;31m        \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 109 \u001b[0;31m        \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    110 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;31m# bse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-112-8768bb56e119>\u001b[0m(110)\u001b[0;36mencode_slot_desc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    109 \u001b[0;31m        \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 110 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;31m# bse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    111 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> desc.shape\n",
      "torch.Size([10, 11, 768])\n",
      "ipdb> q\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-e3983f8b0955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-17-95c0bc59dabf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, batch_size, num_epochs, train_ds, test_ds, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Remove tensorboard logs\")\n",
    "!rm -rf ../data/tensorboard/*\n",
    "\n",
    "print(\"set number of devices\") # not sure we can in jupyter once program already kicked in the first time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "print(\"loading model\")\n",
    "model = CandidateSelector()\n",
    "model = nn.DataParallel(model)\n",
    "model = move_to_device(model, \"cuda\")\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "train_samples = [train_ds[i] for i in range(1000)]\n",
    "test_samples = [test_ds[i] for i in range(100)]\n",
    "\n",
    "print(\"started training\")\n",
    "train(\n",
    "    model=model,\n",
    "    optimizer=optim,\n",
    "    train_ds=train_samples,\n",
    "    test_ds=test_samples,\n",
    "    device=\"cuda\",\n",
    "    num_epochs=20,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_infer(model, test_ds, batch_size, device):\n",
    "#     with torch.no_grad():\n",
    "#         model = move_to_device(model, device)\n",
    "#         model = model.eval()\n",
    "#         test_iter = DialogIterator(test_ds, batch_size, collate_fn=dialogue_mini_batcher)\n",
    "#         with tqdm(test_iter, leave=False) as test_pbar:\n",
    "#             for i, batch in enumerate(test_pbar):\n",
    "#                 batch = move_to_device(batch, device)\n",
    "#                 output = model(**batch)\n",
    "                \n",
    "#                 # results..\n",
    "#                 dial_ids = batch[\"dial_id\"]\n",
    "#                 turn_id = batch[\"turnid\"]\n",
    "#                 serv_id = batch[\"serviceid\"]\n",
    "                \n",
    "#                 mem_loc = torch.argmax(output[\"score\"], dim=-1)\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 outputs.append((batch,out))\n",
    "#         return outputs\n",
    "    \n",
    "# results = run_infer(model, [test_ds[0]], 1, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#b=next(iter(DialogIterator(train_ds, 1, collate_fn=dialogue_mini_batcher)))\n",
    "#b[\"slot_memory_loc\"][\"ids\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "226px",
    "width": "226px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "495px",
    "left": "1510px",
    "right": "97px",
    "top": "135px",
    "width": "403px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

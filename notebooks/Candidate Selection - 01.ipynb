{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as D\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import re\n",
    "import concurrent.futures\n",
    "\n",
    "from copy import deepcopy\n",
    "from functools import lru_cache\n",
    "from collections import OrderedDict\n",
    "from types import SimpleNamespace\n",
    "from collections.abc import Iterable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from ipdb import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        self.bert = bert\n",
    "    \n",
    "    def __call__(self, text, include_sep=True):\n",
    "        tokens = self.bert.tokenize(text)\n",
    "        if include_sep:\n",
    "            tokens.insert(0, \"[CLS]\")\n",
    "            tokens.append(\"[SEP]\")\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "class TokenIndexer:\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        self.bert = bert\n",
    "        \n",
    "    def inv(self, *args, **kw):\n",
    "        # tokens MUST be list, tensor doesn't work.\n",
    "        return self.bert.convert_ids_to_tokens(*args, **kw)\n",
    "        \n",
    "    def __call__(self, *args, **kw):\n",
    "        return self.bert.convert_tokens_to_ids(*args, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def label_binarize(labels, classes):\n",
    "    # labels: np.array or tensor [batch, 1]\n",
    "    # classes: [..] list of classes\n",
    "    # weirdly,`sklearn.preprocessing.label_binarize` returns [1] or [0]\n",
    "    # instead of onehot ONLY when executing in this script!\n",
    "    vectors = [np.zeros(len(classes)) for _ in labels]\n",
    "    for i, label in enumerate(labels):\n",
    "        for j, c in enumerate(classes):\n",
    "            if c == label:\n",
    "                vectors[i][j] = 1\n",
    "    return np.array(vectors)\n",
    "    \n",
    "\n",
    "def label_inv_binarize(vectors, classes):\n",
    "    # labels: np.array or tensor [batch, classes]\n",
    "    # classes: [..] list of classes\n",
    "    # follows sklearn LabelBinarizer.inverse_transform()\n",
    "    # given all zeros, predicts label at index 0, instead of returning none!\n",
    "    # sklearn doesn't have functional API of inverse transform\n",
    "    labels = []\n",
    "    for each in vectors:\n",
    "        index = np.argmax(each)\n",
    "        labels.append(classes[index])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def padded_array(array, value=0):\n",
    "    # TODO: this does not do type checking; and wow it can be slow on strings.\n",
    "    # expects array to have fixed _number_ of dimensions\n",
    "    \n",
    "    # resolve the shape of padded array\n",
    "    shape_index = {}\n",
    "    queue = [(array, 0)]\n",
    "    while queue:\n",
    "        subarr, dim = queue.pop(0)\n",
    "        shape_index[dim] = max(shape_index.get(dim, -1), len(subarr))\n",
    "        for x in subarr:\n",
    "            if isinstance(x, Iterable) and not isinstance(x, str):\n",
    "                queue.append((x, dim+1))\n",
    "    shape = [shape_index[k] for k in range(max(shape_index) + 1)]\n",
    "    \n",
    "    # fill the values \n",
    "    padded = np.ones(shape) * value\n",
    "    queue = [(array, [])]\n",
    "    while queue:\n",
    "        subarr, index = queue.pop(0)\n",
    "        for j, x in enumerate(subarr):\n",
    "            if isinstance(x, Iterable):\n",
    "                queue.append((x, index + [j]))\n",
    "            else:\n",
    "                padded[tuple(index + [j])] = x\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Schemas(object):\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath) as f:\n",
    "            self.index = {}\n",
    "            for schema in json.load(f):\n",
    "                service_name = schema[\"service_name\"]\n",
    "                self.index[service_name] = schema\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get(self, service):\n",
    "        result = dict(\n",
    "            # service\n",
    "            name=service,\n",
    "            desc=self.index[service][\"description\"],\n",
    "            \n",
    "            # slots\n",
    "            slot_name=[],\n",
    "            slot_desc=[],\n",
    "            slot_iscat=[], \n",
    "            slot_vals=[], # collected only for cat slots.. not sure if that makes sense\n",
    "\n",
    "            # intents\n",
    "            intent_name=[],\n",
    "            intent_desc=[],\n",
    "            intent_istrans=[],\n",
    "            intent_reqslots=[],\n",
    "            intent_optslots=[],\n",
    "            intent_optvals=[],\n",
    "        )\n",
    "\n",
    "        for slot in self.index[service][\"slots\"]:\n",
    "            result[\"slot_name\"].append(slot[\"name\"])\n",
    "            result[\"slot_desc\"].append(slot[\"description\"])\n",
    "            result[\"slot_iscat\"].append(slot[\"is_categorical\"])\n",
    "            result[\"slot_vals\"].append(slot[\"possible_values\"])\n",
    "        \n",
    "        for intent in self.index[service][\"intents\"]:\n",
    "            result[\"intent_name\"].append(intent[\"name\"])\n",
    "            result[\"intent_desc\"].append(intent[\"description\"])\n",
    "            result[\"intent_istrans\"].append(intent[\"is_transactional\"])\n",
    "            result[\"intent_reqslots\"].append(intent[\"required_slots\"])\n",
    "            result[\"intent_optslots\"].append(list(intent[\"optional_slots\"].keys()))\n",
    "            result[\"intent_optvals\"].append(list(intent[\"optional_slots\"].values()))\n",
    "\n",
    "        return result    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialogue Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueDataset(D.Dataset):\n",
    "    def __init__(self, filename, schemas, tokenizer, token_indexer):\n",
    "        with open(filename) as f:\n",
    "            self.ds = json.load(f)\n",
    "        self.schemas = schemas\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexer = token_indexer\n",
    "        self.dialogues = []\n",
    "        for dial in self.ds:\n",
    "            fields = self.dial_to_fields(dial)\n",
    "            self.dialogues.append(fields)\n",
    "        self.schemas = None\n",
    "        self.tokenizer = None\n",
    "        self.token_indexer = None\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dialogues[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "    \n",
    "    def fd_serv_name(self, dial, fields):\n",
    "        resp = dict(value=[])\n",
    "        for service in dial[\"services\"]:\n",
    "            resp[\"value\"].append(service)\n",
    "        return resp\n",
    "        \n",
    "    def fd_serv_desc(self, dial, fields):\n",
    "        resp = dict(value=[], tokens=[], ids=[], mask=[])\n",
    "        for service in dial[\"services\"]:\n",
    "            desc = self.schemas.get(service)[\"desc\"]\n",
    "            resp[\"value\"].append(desc)\n",
    "            resp[\"tokens\"].append(self.tokenizer(desc))\n",
    "            resp[\"ids\"].append(self.token_indexer(resp[\"tokens\"][-1]))\n",
    "            resp[\"mask\"].append([1] * len(resp[\"tokens\"][-1]))\n",
    "        return resp\n",
    "    \n",
    "    def fd_slot_name(self, dial, fields):\n",
    "        resp = {\"value\": []}\n",
    "        for serv in dial[\"services\"]:\n",
    "            schema = self.schemas.get(serv)\n",
    "            resp[\"value\"].append(schema[\"slot_name\"])\n",
    "        return resp\n",
    "    \n",
    "    def fd_slot_desc(self, dial, fields):\n",
    "        resp = dict(value=[], tokens=[], ids=[], mask=[])\n",
    "        for serv in dial[\"services\"]:\n",
    "            s_desc = self.schemas.get(serv)[\"slot_desc\"]\n",
    "            s_tokens = [self.tokenizer(d) for d in s_desc]\n",
    "            s_ids = [self.token_indexer(d) for d in s_tokens]\n",
    "            s_mask = [[1] * len(d) for d in s_tokens]\n",
    "            resp[\"value\"].append(s_desc)\n",
    "            resp[\"tokens\"].append(s_tokens)\n",
    "            resp[\"ids\"].append(s_ids)\n",
    "            resp[\"mask\"].append(s_mask)\n",
    "        return resp\n",
    "\n",
    "    def fd_slot_memory(self, dial, fields):\n",
    "        # memory is a sequence of slot tagger values across all frames in a dial.. that grows per turn\n",
    "        # maintain snapshot at each turn\n",
    "        resp = dict(\n",
    "            value=[], # unfeaturized memory\n",
    "            tokens=[], # tokenized memory [turn, mem-size, tokens]\n",
    "            ids=[], # indexed memory [turn mem-size, tokens]\n",
    "            mask=[], # mask on memory [turn, mem-size, tokens]\n",
    "            ids_memsize=[] # mem sizes [turn, 1]\n",
    "        )\n",
    "        \n",
    "        # memory is sequential, initialized by vals from schema, only has values\n",
    "        memory = [\"NONE\", \"dontcare\"] # keep these at index 0, 1\n",
    "        memory_index = set()\n",
    "        for serv in dial[\"services\"]:\n",
    "            schema = self.schemas.get(serv)\n",
    "            for values in schema[\"slot_vals\"]:\n",
    "                for val in values:\n",
    "                    if val not in memory_index:\n",
    "                        memory.append(val)\n",
    "                        memory_index.add(val)\n",
    "        \n",
    "        # at each user turn create a memory snapshot..\n",
    "        for turn in dial[\"turns\"]:\n",
    "            memory = deepcopy(memory)\n",
    "            # pick slot tagger's ground truth ; categorical slot vals are already initialized!\n",
    "            utter = turn[\"utterance\"]\n",
    "            for frame in turn[\"frames\"]:\n",
    "                for tag in frame[\"slots\"]:\n",
    "                    st, en = tag[\"start\"], tag[\"exclusive_end\"]\n",
    "                    value = utter[st:en]\n",
    "                    if value not in memory_index:\n",
    "                        memory.append(value)\n",
    "                        memory_index.add(value)\n",
    "            if turn[\"speaker\"] == \"USER\":\n",
    "                resp[\"value\"].append(memory)\n",
    "                resp[\"ids_memsize\"].append(len(memory))\n",
    "\n",
    "        # tokenize and index the memory values\n",
    "        # value: [turn, values], ids/tokens: [turn, values, tokens]\n",
    "        for mem_snapshot in resp[\"value\"]:\n",
    "            mem_tokens = []\n",
    "            mem_ids = []\n",
    "            mem_mask = []\n",
    "            for val in mem_snapshot:\n",
    "                tokens = self.tokenizer(val)\n",
    "                mem_tokens.append(tokens)\n",
    "                mem_ids.append(self.token_indexer(tokens))\n",
    "                mem_mask.append([1] * len(tokens))\n",
    "            resp[\"tokens\"].append(mem_tokens)\n",
    "            resp[\"ids\"].append(mem_ids)\n",
    "            resp[\"mask\"].append(mem_mask)\n",
    "        \n",
    "        return resp\n",
    "    \n",
    "    def fd_slot_memory_loc(self, dial, fields):\n",
    "        resp = dict(\n",
    "            value=[],  # string value\n",
    "            ids=[], # memory loc\n",
    "            ids_onehot=[], # onehot of memory loc\n",
    "            mask=[],\n",
    "            mask_onehot=[],\n",
    "            mask_none=[], # 1 -> non NONE values\n",
    "            mask_none_onehot=[],\n",
    "        )\n",
    "        \n",
    "        # query dialog memory snapshots per turn. NOTE: fd_slot_memory should exec first.\n",
    "        memory = fields[\"slot_memory\"][\"value\"]\n",
    "        \n",
    "        # init snapshot: service, slot -> memory loc, val\n",
    "        memory_loc = []\n",
    "        memory_val = []\n",
    "        for turn in dial[\"turns\"]:\n",
    "            if turn[\"speaker\"] == \"USER\":\n",
    "                loc = OrderedDict()\n",
    "                val = OrderedDict()\n",
    "                for serv in dial[\"services\"]:\n",
    "                    loc[serv] = OrderedDict()\n",
    "                    val[serv] = OrderedDict()\n",
    "                    for slot in self.schemas.get(serv)[\"slot_name\"]:\n",
    "                        loc[serv][slot] = None\n",
    "                        val[serv][slot] = None\n",
    "                memory_loc.append(loc)\n",
    "                memory_val.append(val)\n",
    "        \n",
    "        # fill the memory locations \n",
    "        snapshot_id = 0\n",
    "        for turn in dial[\"turns\"]:\n",
    "            if turn[\"speaker\"] == \"USER\":\n",
    "                turn_memory = memory[snapshot_id]\n",
    "                turn_memory_loc = memory_loc[snapshot_id]\n",
    "                turn_memory_val = memory_val[snapshot_id]\n",
    "                for frame in turn[\"frames\"]:\n",
    "                    service = frame[\"service\"]\n",
    "                    for slot, values in frame[\"state\"][\"slot_values\"].items():\n",
    "                        val = re.sub(\"\\u2013\", \"-\", values[0]) # dial 59_00125 turn 14\n",
    "                        turn_memory_loc[service][slot] = turn_memory.index(val)\n",
    "                        turn_memory_val[service][slot] = val\n",
    "                    # add locations to UNKNOWN slots\n",
    "                    for slot, val in turn_memory_loc[service].items():\n",
    "                        if val is None:\n",
    "                            turn_memory_loc[service][slot] = turn_memory.index(\"NONE\")\n",
    "                            turn_memory_val[service][slot] = \"NONE\"\n",
    "                snapshot_id += 1\n",
    "        \n",
    "        # featurize\n",
    "        snapshot_id = 0\n",
    "        for turn in dial[\"turns\"]:\n",
    "            if turn[\"speaker\"] == \"USER\":\n",
    "                turn_memory = memory[snapshot_id]\n",
    "                turn_memory_loc = memory_loc[snapshot_id]\n",
    "                turn_memory_val = memory_val[snapshot_id]\n",
    "                none_loc = memory[snapshot_id].index(\"NONE\")\n",
    "                turn_fields = dict(\n",
    "                    value=[], ids=[], ids_onehot=[], mask=[], mask_onehot=[],\n",
    "                    mask_none=[], mask_none_onehot=[],\n",
    "                )\n",
    "\n",
    "                for serv in turn_memory_loc:\n",
    "                    mem_size = len(turn_memory)\n",
    "                    vals = list(turn_memory_val[serv].values())\n",
    "                    ids = list(turn_memory_loc[serv].values())\n",
    "                    ids_onehots = label_binarize(ids, list(range(mem_size)))\n",
    "                    mask = [1] * len(ids)\n",
    "                    mask_onehots = [[1] * mem_size for _ in ids]\n",
    "\n",
    "                    mask_none = [int(v!=\"NONE\") for v in vals]\n",
    "                    mask_none_onehot = [[1] * mem_size for _ in ids]\n",
    "                    for v, loc, onehot in zip(vals, ids, mask_none_onehot):\n",
    "                        if v == \"NONE\":\n",
    "                            onehot[loc] = 0\n",
    "                                \n",
    "                    turn_fields[\"value\"].append(vals)\n",
    "                    turn_fields[\"ids\"].append(ids)\n",
    "                    turn_fields[\"ids_onehot\"].append(ids_onehots)\n",
    "                    turn_fields[\"mask\"].append(mask)\n",
    "                    turn_fields[\"mask_onehot\"].append(mask_onehots)\n",
    "                    turn_fields[\"mask_none\"].append(mask_none)\n",
    "                    turn_fields[\"mask_none_onehot\"].append(mask_none_onehot)\n",
    "\n",
    "                # update turn\n",
    "                for k, v in turn_fields.items():\n",
    "                    resp[k].append(v)\n",
    "                \n",
    "                snapshot_id += 1\n",
    "            \n",
    "        return resp\n",
    "    \n",
    "    def fd_num_turns(self, dial, fields):\n",
    "        return {\"ids\": len(dial[\"turns\"])}\n",
    "    \n",
    "    def fd_num_frames(self, dial, fields):\n",
    "        return {\"ids\": [len(t[\"frames\"]) for t in dial[\"turns\"]]}\n",
    "    \n",
    "    def fd_usr_utter(self, dial, fields):\n",
    "        resp = dict(value=[], ids=[], mask=[], tokens=[])\n",
    "        for turn in dial[\"turns\"]:\n",
    "            if turn[\"speaker\"] == \"USER\":\n",
    "                utter = turn[\"utterance\"]\n",
    "                tokens = self.tokenizer(utter)\n",
    "                ids = self.token_indexer(tokens)\n",
    "                resp[\"value\"].append(utter)\n",
    "                resp[\"ids\"].append(ids)\n",
    "                resp[\"tokens\"].append(tokens)\n",
    "                resp[\"mask\"].append([1] * len(tokens))\n",
    "        return resp\n",
    "    \n",
    "    def fd_sys_utter(self, dial, fields):\n",
    "        resp = dict(value=[], ids=[], mask=[], tokens=[])\n",
    "        for turn in dial[\"turns\"]:\n",
    "            if turn[\"speaker\"] == \"SYSTEM\":\n",
    "                utter = turn[\"utterance\"]\n",
    "                tokens = self.tokenizer(utter)\n",
    "                ids = self.token_indexer(tokens)\n",
    "                resp[\"value\"].append(utter)\n",
    "                resp[\"ids\"].append(ids)\n",
    "                resp[\"tokens\"].append(tokens)\n",
    "                resp[\"mask\"].append([1] * len(tokens))\n",
    "        return resp\n",
    "    \n",
    "    def fd_dial_id(self, dial, fields):\n",
    "        return {\"value\": dial[\"dialogue_id\"]}\n",
    "    \n",
    "    def dial_to_fields(self, dial):\n",
    "        fields = {}\n",
    "        ordered_funcs = [\n",
    "            \"fd_dial_id\",\n",
    "            \"fd_num_turns\", \"fd_num_frames\",\n",
    "            \"fd_serv_name\", \"fd_serv_desc\", \n",
    "            \"fd_slot_name\", \"fd_slot_desc\",\n",
    "            \"fd_slot_memory\", \"fd_slot_memory_loc\",\n",
    "            \"fd_usr_utter\", \"fd_sys_utter\"]\n",
    "        for func in ordered_funcs:\n",
    "            name = func.split(\"fd_\", maxsplit=1)[-1]\n",
    "            value = getattr(self, func)(dial, fields)\n",
    "            if value is not None:\n",
    "                fields[name] = value\n",
    "        return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "bert_ = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = Tokenizer(bert_)\n",
    "token_indexer = TokenIndexer(bert_)\n",
    "\n",
    "train_schemas = Schemas(\"../data/train/schema.json\")\n",
    "test_schemas = Schemas(\"../data/dev/schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41aa0658dd8f484997c9e45e79b148f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=127), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "train_dial_sets = []\n",
    "train_dial_files = sorted(glob.glob(\"../data/train/dialogues*.json\"))\n",
    "num_workers = min(20, len(train_dial_files))\n",
    "\n",
    "def worker(filename):\n",
    "    return DialogueDataset(filename, train_schemas, tokenizer, token_indexer)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "    for ds in tqdm(executor.map(worker, train_dial_files), total=len(train_dial_files)):\n",
    "        train_dial_sets.append(ds)\n",
    "\n",
    "train_ds = D.ConcatDataset(train_dial_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29bd30cd6004e938522a348d21c0ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load test dataset\n",
    "test_dial_sets = []\n",
    "test_dial_files = sorted(glob.glob(\"../data/dev/dialogues*.json\"))\n",
    "num_workers = min(20, len(train_dial_files))\n",
    "\n",
    "def worker(filename):\n",
    "    return DialogueDataset(filename, test_schemas, tokenizer, token_indexer)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "    for ds in tqdm(executor.map(worker, test_dial_files), total=len(test_dial_files)):\n",
    "        test_dial_sets.append(ds)\n",
    "\n",
    "test_ds = D.ConcatDataset(test_dial_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Metrics(OrderedDict):\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        formatted = []\n",
    "        for k, v in self.items():\n",
    "            if type(v) is float:\n",
    "                v = round(v, 4)\n",
    "            formatted.append((k, v))\n",
    "        return \", \".join(\"{}: {}\".format(k, v) for k, v in formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def move_to_device(obj, device):\n",
    "    if type(obj) is list:\n",
    "        return [move_to_device(o, device) for o in obj]\n",
    "    elif type(obj) is dict:\n",
    "        return {k: move_to_device(v, device) for k, v in obj.items()}\n",
    "    elif type(obj) is torch.Tensor or isinstance(obj, nn.Module):\n",
    "        return obj.to(device)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dialogue_mini_batcher(dialogues):\n",
    "    default_padding = 0\n",
    "    batch = {}\n",
    "    for dial in dialogues:\n",
    "        # populate the batch\n",
    "        for field, data in dial.items():\n",
    "            if field not in batch:\n",
    "                batch[field] = {}\n",
    "            for attr, val in data.items():\n",
    "                if attr == \"padding\":\n",
    "                    batch[field][attr] = val\n",
    "                else:\n",
    "                    batch[field][attr] = batch[field].get(attr, [])\n",
    "                    batch[field][attr].append(val)\n",
    "\n",
    "    # padding on field attributes\n",
    "    for field_name, data in batch.items():\n",
    "        for attr in data:\n",
    "            if attr.startswith(\"ids\") or attr.startswith(\"mask\"):\n",
    "                if type(data[attr]) is not torch.Tensor: # don't move b/w CPU and GPU unnecessarily\n",
    "                    data[attr] = padded_array(data[attr], default_padding)\n",
    "                    data[attr] = torch.tensor(data[attr], device=\"cpu\")\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "class DialogIterator(object):\n",
    "    \"\"\"A simple wrapper on DataLoader\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size, *args, **kw):\n",
    "        self.length = None\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.iterator = D.DataLoader(dataset, batch_size, *args, **kw)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.length is None:\n",
    "            self.length = 0\n",
    "            for dial in self.dataset:\n",
    "                self.length += dial[\"num_turns\"][\"ids\"] + len(dial[\"serv_name\"][\"value\"])\n",
    "            self.length = int(self.length / self.batch_size)\n",
    "        return self.length\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.iterator:\n",
    "            num_turns = batch[\"usr_utter\"][\"ids\"].shape[1]\n",
    "            num_services = batch[\"serv_desc\"][\"ids\"].shape[1]\n",
    "            for turnid in range(num_turns):\n",
    "                for sid in range(num_services):\n",
    "                    inputs = dict(turnid=turnid, serviceid=sid)\n",
    "                    inputs.update(batch)\n",
    "                    yield inputs\n",
    "                    \n",
    "#next(iter(DialogIterator(train_ds, 1, collate_fn=dialogue_mini_batcher)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_turns ids --> torch.Size([1])\n",
      "num_frames ids --> torch.Size([1, 24])\n",
      "serv_desc ids --> torch.Size([1, 1, 10])\n",
      "serv_desc mask --> torch.Size([1, 1, 10])\n",
      "slot_desc ids --> torch.Size([1, 1, 11, 12])\n",
      "slot_desc mask --> torch.Size([1, 1, 11, 12])\n",
      "slot_memory ids --> torch.Size([1, 12, 29, 10])\n",
      "slot_memory mask --> torch.Size([1, 12, 29, 10])\n",
      "slot_memory ids_memsize --> torch.Size([1, 12])\n",
      "slot_memory_loc ids --> torch.Size([1, 12, 1, 11])\n",
      "slot_memory_loc ids_onehot --> torch.Size([1, 12, 1, 11, 29])\n",
      "slot_memory_loc mask --> torch.Size([1, 12, 1, 11])\n",
      "slot_memory_loc mask_onehot --> torch.Size([1, 12, 1, 11, 29])\n",
      "slot_memory_loc mask_none --> torch.Size([1, 12, 1, 11])\n",
      "slot_memory_loc mask_none_onehot --> torch.Size([1, 12, 1, 11, 29])\n",
      "usr_utter ids --> torch.Size([1, 12, 25])\n",
      "usr_utter mask --> torch.Size([1, 12, 25])\n",
      "sys_utter ids --> torch.Size([1, 12, 31])\n",
      "sys_utter mask --> torch.Size([1, 12, 31])\n"
     ]
    }
   ],
   "source": [
    "def print_shapes(ds):\n",
    "    it = next(iter(DialogIterator(ds, 1, collate_fn=dialogue_mini_batcher)))\n",
    "    for field, val in it.items():\n",
    "        if type(val) is dict:\n",
    "            for attr in val:\n",
    "                if attr.startswith(\"ids\") or attr.startswith(\"mask\"):\n",
    "                    print(field, attr, \"-->\", val[attr].shape)\n",
    "                    \n",
    "                    \n",
    "print_shapes(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import PretrainedBertEmbedder\n",
    "from allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateSelector(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = PretrainedBertEmbedder(\"bert-base-uncased\", requires_grad=False, top_layer_only=True)\n",
    "        \n",
    "        # encode utter and desc tokens\n",
    "        self.l0 = nn.GRU(self.emb.output_dim, self.emb.output_dim, batch_first=True, num_layers=1, bidirectional=True)\n",
    "        \n",
    "        # encode memory cells and rank \n",
    "        self.l1 = nn.GRU(self.emb.output_dim, self.emb.output_dim, batch_first=True, num_layers=1, bidirectional=True)\n",
    "        self.l2 = nn.Linear(self.emb.output_dim, 1, bias=False)\n",
    "        \n",
    "        # encode slot desc\n",
    "        self.l3 = nn.GRU(self.emb.output_dim, self.emb.output_dim, batch_first=True, num_layers=1, bidirectional=True)\n",
    "        \n",
    "        # metrics\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "        # init weights: classifier's performance changes heavily on these\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.startswith(\"l0.\") or name.startswith(\"l1.\") or name.startswith(\"l3.\"):\n",
    "                print(\"Initializing bias/weights of \", name)\n",
    "                if \"weight\" in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    param.data.fill_(0.)\n",
    "\n",
    "    def get_metrics(self, reset=False):\n",
    "        return {\"acc\": self.accuracy.get_metric(reset)}\n",
    "    \n",
    "    def compute_score1(self, utter_h, slot_desc_h, memory_h):\n",
    "        s = slot_desc_h.shape[1]\n",
    "        m = memory_h.shape[1]\n",
    "\n",
    "        # [b,s,m,e]\n",
    "        utter_h = utter_h.unsqueeze(1).unsqueeze(1).expand(-1,s,m,-1)\n",
    "        slot_desc_h = slot_desc_h.unsqueeze(2).expand(-1,-1,m,-1)\n",
    "        memory_h = memory_h.unsqueeze(1).expand(-1,s,-1,-1)\n",
    "        \n",
    "        energy = utter_h * slot_desc_h * memory_h\n",
    "        attn = self.l2(energy).squeeze(-1)\n",
    "        attn = F.softmax(attn, dim=-1) # b,s,m\n",
    "        \n",
    "        return attn\n",
    "        \n",
    "    def compute_score2(self, utter_h, slot_desc_h, memory_h):\n",
    "        context = (utter_h.unsqueeze(1) * slot_desc_h).unsqueeze(1) # [b,1,s,e]\n",
    "        memory_h = memory_h.unsqueeze(2) # [b,m,1,e]\n",
    "        energy = context * memory_h # [b,m,s,e]\n",
    "        attn = self.l2(energy).squeeze(-1).permute(0,2,1) # [b,s,m]\n",
    "        score = F.softmax(attn, dim=-1)\n",
    "        return score\n",
    "    \n",
    "    # this works!!\n",
    "    def compute_score3(self, utter_h, slot_desc_h, memory_h):\n",
    "        num_slots = slot_desc_h.shape[1]\n",
    "        score = []\n",
    "        for s in range(num_slots):\n",
    "            context = (utter_h * slot_desc_h[:,s,:]).unsqueeze(-1) # [b,e,1]\n",
    "            attn = torch.bmm(memory_h, context).squeeze(-1) # [b,m,e] * [b,e,1] -> [b,m,1] -> [b,m]\n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "            score.append(attn)\n",
    "        score = torch.stack(score, dim=1) # [b,s,m]\n",
    "        return score\n",
    "    \n",
    "    def compute_score4(self, utter_h, slot_desc_h, memory_h):\n",
    "        num_slots = slot_desc_h.shape[1]\n",
    "        score = []\n",
    "        for s in range(num_slots):\n",
    "            context = (utter_h * slot_desc_h[:,s,:]).unsqueeze(1) # [b,1,e]\n",
    "            attn = self.l2(memory_h * context).squeeze(-1) # [b,m]\n",
    "            score.append(attn)\n",
    "        score = torch.stack(score, dim=1) # [b,s,m]\n",
    "        score = F.softmax(score, dim=-1)\n",
    "        return score\n",
    "    \n",
    "    def compute_score5(self, utter_h, slot_desc_h, memory_h):\n",
    "        score = torch.einsum(\"be,bse,bme->bsme\", utter_h, slot_desc_h, memory_h)\n",
    "        score = self.l2(score).squeeze(-1)\n",
    "        score = F.softmax(score, dim=-1)\n",
    "        return score\n",
    "\n",
    "    def forward(self, **batch):\n",
    "        turnid = batch[\"turnid\"]\n",
    "        serviceid = batch[\"serviceid\"]\n",
    "\n",
    "        # encode user utter\n",
    "        sys_utter = batch[\"sys_utter\"][\"ids\"][:,turnid,:]\n",
    "        usr_utter = batch[\"usr_utter\"][\"ids\"][:,turnid,:]\n",
    "        utter = torch.cat([sys_utter, usr_utter], dim=-1)\n",
    "        utter = self.emb(utter.long()) # [batch, seq, emb]\n",
    "        utter, utter_h = self.l0(utter) # [batch, seq, emb * dir] [layers * dir, batch, emb]\n",
    "        utter_h = utter_h[-1] # [batch, emb]\n",
    "        \n",
    "        # encode slot descs\n",
    "        slot_desc = batch[\"slot_desc\"][\"ids\"][:,serviceid,:].long()\n",
    "        slot_desc = self.emb(slot_desc)  # [batch, slots, tokens, emb]\n",
    "        sh = slot_desc.shape\n",
    "        slot_desc = slot_desc.view(sh[0] * sh[1], sh[2], -1) # [batch * slots, tokens, emb]\n",
    "        slot_desc, slot_desc_h = self.l3(slot_desc)\n",
    "        slot_desc_h = slot_desc_h[-1].view(sh[0], sh[1], -1) # [batch, slots, emb]\n",
    "            \n",
    "        # encode memory snapshot\n",
    "        memory = batch[\"slot_memory\"][\"ids\"][:,turnid,:] # [batch, memory, tokens]\n",
    "        memory = self.emb(memory.long()) \n",
    "        sh = memory.shape\n",
    "        memory = memory.view(sh[0] * sh[1], sh[2], sh[3])\n",
    "        memory, memory_h = self.l1(memory)\n",
    "        memory_h = memory_h[-1].view(sh[0], sh[1], -1) # [batch, memory, emb]\n",
    "        \n",
    "        score = self.compute_score3(utter_h, slot_desc_h, memory_h)\n",
    "        output = {\"score\": score}\n",
    "\n",
    "        if \"slot_memory_loc\" in batch:\n",
    "            # calc loss\n",
    "            target_score = batch[\"slot_memory_loc\"][\"ids_onehot\"][:,turnid,serviceid,:].contiguous().view(-1, 1) # [batch, slots, memory]\n",
    "            target_mask = batch[\"slot_memory_loc\"][\"mask_onehot\"][:,turnid,serviceid,:].contiguous().view(-1, 1)\n",
    "            output[\"loss\"] = F.binary_cross_entropy(score.view(-1,1), target_score.float(), target_mask.float())\n",
    "            output[\"loss\"] = output[\"loss\"].unsqueeze(0) # don't return scalar.\n",
    "            \n",
    "            # calc acc. don't calc acc on slots that are predicted NONE.\n",
    "            target_score = batch[\"slot_memory_loc\"][\"ids\"][:,turnid,serviceid,:] # [batch, slots]\n",
    "            target_mask = batch[\"slot_memory_loc\"][\"mask\"][:,turnid,serviceid,:]\n",
    "            target_mask_none = batch[\"slot_memory_loc\"][\"mask_none\"][:,turnid,serviceid,:]\n",
    "            self.accuracy(score, target_score.float(), (target_mask * target_mask_none).float())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def module(m):\n",
    "    if type(m) is nn.DataParallel:\n",
    "        return m.module\n",
    "    return m\n",
    "\n",
    "def train(model, optimizer, batch_size, num_epochs, train_ds, test_ds, device):\n",
    "    model = move_to_device(model, device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # train\n",
    "        metrics = Metrics(epoch=epoch, run=\"train\")\n",
    "        model = model.train()\n",
    "        train_iter = DialogIterator(train_ds, batch_size, collate_fn=dialogue_mini_batcher)\n",
    "        with tqdm(train_iter, total=len(train_iter), leave=epoch==num_epochs-1) as train_pbar:\n",
    "            for i, batch in enumerate(train_pbar):\n",
    "                batch = move_to_device(batch, device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(**batch)\n",
    "                output[\"loss\"] = output[\"loss\"].mean()\n",
    "                output[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "                # avg metrics per dialog\n",
    "                if batch[\"turnid\"] == 0:\n",
    "                    metrics.update(module(model).get_metrics(reset=True))\n",
    "                    metrics[\"loss\"] = output[\"loss\"].item()\n",
    "                    train_pbar.set_description(str(metrics))\n",
    "        \n",
    "        # test\n",
    "        metrics = Metrics(epoch=epoch, run=\"test\")\n",
    "        if test_ds:\n",
    "            test_iter = DialogIterator(test_ds, batch_size, collate_fn=dialogue_mini_batcher)\n",
    "            with tqdm(test_iter, total=len(test_iter), leave=epoch==num_epochs-1) as test_pbar:\n",
    "                with torch.no_grad():\n",
    "                    model = model.eval()\n",
    "                    for i, batch in enumerate(test_pbar):\n",
    "                        batch = move_to_device(batch, device)\n",
    "                        output = model(**batch)\n",
    "                        output[\"loss\"] = output[\"loss\"].mean()\n",
    "                        if batch[\"turnid\"] == 0:\n",
    "                            metrics.update(module(model).get_metrics(reset=True))\n",
    "                            metrics[\"loss\"] = output[\"loss\"].item()\n",
    "                            test_pbar.set_description(str(metrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "Initializing bias/weights of  l0.weight_ih_l0\n",
      "Initializing bias/weights of  l0.weight_hh_l0\n",
      "Initializing bias/weights of  l0.bias_ih_l0\n",
      "Initializing bias/weights of  l0.bias_hh_l0\n",
      "Initializing bias/weights of  l0.weight_ih_l0_reverse\n",
      "Initializing bias/weights of  l0.weight_hh_l0_reverse\n",
      "Initializing bias/weights of  l0.bias_ih_l0_reverse\n",
      "Initializing bias/weights of  l0.bias_hh_l0_reverse\n",
      "Initializing bias/weights of  l1.weight_ih_l0\n",
      "Initializing bias/weights of  l1.weight_hh_l0\n",
      "Initializing bias/weights of  l1.bias_ih_l0\n",
      "Initializing bias/weights of  l1.bias_hh_l0\n",
      "Initializing bias/weights of  l1.weight_ih_l0_reverse\n",
      "Initializing bias/weights of  l1.weight_hh_l0_reverse\n",
      "Initializing bias/weights of  l1.bias_ih_l0_reverse\n",
      "Initializing bias/weights of  l1.bias_hh_l0_reverse\n",
      "Initializing bias/weights of  l3.weight_ih_l0\n",
      "Initializing bias/weights of  l3.weight_hh_l0\n",
      "Initializing bias/weights of  l3.bias_ih_l0\n",
      "Initializing bias/weights of  l3.bias_hh_l0\n",
      "Initializing bias/weights of  l3.weight_ih_l0_reverse\n",
      "Initializing bias/weights of  l3.weight_hh_l0_reverse\n",
      "Initializing bias/weights of  l3.bias_ih_l0_reverse\n",
      "Initializing bias/weights of  l3.bias_hh_l0_reverse\n",
      "started training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600714e96370446095526af95bff7b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=504), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"loading model\")\n",
    "model = CandidateSelector()\n",
    "model = nn.DataParallel(model)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "train_samples = [train_ds[i] for i in range(1000)]\n",
    "test_samples = [test_ds[i] for i in range(100)]\n",
    "\n",
    "print(\"started training\")\n",
    "train(\n",
    "    model=model,\n",
    "    optimizer=optim,\n",
    "    train_ds=train_samples,\n",
    "    test_ds=test_samples,\n",
    "    device=\"cuda\",\n",
    "    num_epochs=100,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92f1d934cfb42918b20887c541e787a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def run_infer(model, test_ds, batch_size, device):\n",
    "#     with torch.no_grad():\n",
    "#         model = move_to_device(model, device)\n",
    "#         model = model.eval()\n",
    "#         test_iter = DialogIterator(test_ds, batch_size, collate_fn=dialogue_mini_batcher)\n",
    "#         with tqdm(test_iter, leave=False) as test_pbar:\n",
    "#             for i, batch in enumerate(test_pbar):\n",
    "#                 batch = move_to_device(batch, device)\n",
    "#                 output = model(**batch)\n",
    "                \n",
    "#                 # results..\n",
    "#                 dial_ids = batch[\"dial_id\"]\n",
    "#                 turn_id = batch[\"turnid\"]\n",
    "#                 serv_id = batch[\"serviceid\"]\n",
    "                \n",
    "#                 mem_loc = torch.argmax(output[\"score\"], dim=-1)\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 outputs.append((batch,out))\n",
    "#         return outputs\n",
    "    \n",
    "# results = run_infer(model, [test_ds[0]], 1, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#b=next(iter(DialogIterator(train_ds, 1, collate_fn=dialogue_mini_batcher)))\n",
    "#b[\"slot_memory_loc\"][\"ids\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "226px",
    "width": "226px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "495px",
    "left": "1510px",
    "right": "97px",
    "top": "135px",
    "width": "403px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

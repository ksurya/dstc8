{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as D\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import concurrent.futures\n",
    "\n",
    "from functools import lru_cache\n",
    "from collections import OrderedDict\n",
    "from types import SimpleNamespace\n",
    "from collections.abc import Iterable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from ipdb import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Schemas(object):\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath) as f:\n",
    "            self.index = {}\n",
    "            for schema in json.load(f):\n",
    "                service_name = schema[\"service_name\"]\n",
    "                self.index[service_name] = schema\n",
    "\n",
    "    def get_service_desc(self, service):\n",
    "        return self.index[service][\"description\"]\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_slot_desc(self, service, slot):\n",
    "        for item in self.index[service][\"slots\"]:\n",
    "            if item[\"name\"] == slot:\n",
    "                return item[\"description\"]\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_intent_desc(self, service, intent):\n",
    "        for item in self.index[service][\"intents\"]:\n",
    "            if item[\"name\"] == intent:\n",
    "                return item[\"description\"]\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get(self, service):\n",
    "        result = dict(\n",
    "            # service\n",
    "            service_name=service,\n",
    "            service_desc=self.index[service][\"description\"],\n",
    "            \n",
    "            # slots\n",
    "            slot_name=[],\n",
    "            slot_desc=[],\n",
    "            slot_iscat=[], \n",
    "            slot_vals=[], # collected only for cat slots.. not sure if that makes sense\n",
    "\n",
    "            # intents\n",
    "            intent_name=[],\n",
    "            intent_desc=[],\n",
    "            intent_istrans=[],\n",
    "            intent_reqslots=[],\n",
    "            intent_optslots=[],\n",
    "            intent_optvals=[],\n",
    "        )\n",
    "\n",
    "        for slot in self.index[service][\"slots\"]:\n",
    "            result[\"slot_name\"].append(slot[\"name\"])\n",
    "            result[\"slot_desc\"].append(slot[\"description\"])\n",
    "            result[\"slot_iscat\"].append(slot[\"is_categorical\"])\n",
    "            result[\"slot_vals\"].append(slot[\"possible_values\"])\n",
    "        \n",
    "        for intent in self.index[service][\"intents\"]:\n",
    "            result[\"intent_name\"].append(intent[\"name\"])\n",
    "            result[\"intent_desc\"].append(intent[\"description\"])\n",
    "            result[\"intent_istrans\"].append(intent[\"is_transactional\"])\n",
    "            result[\"intent_reqslots\"].append(intent[\"required_slots\"])\n",
    "            result[\"intent_optslots\"].append(list(intent[\"optional_slots\"].keys()))\n",
    "            result[\"intent_optvals\"].append(list(intent[\"optional_slots\"].values()))\n",
    "\n",
    "        return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        self.bert = bert\n",
    "        \n",
    "    def __call__(self, text, include_sos=True):\n",
    "        tokens = self.bert.tokenize(text)\n",
    "        if include_sos:\n",
    "            tokens.insert(0, \"[CLS]\")\n",
    "            tokens.append(\"[SEP]\")\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "class TokenIndexer:\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        self.bert = bert\n",
    "        \n",
    "    def __call__(self, *args, **kw):\n",
    "        return self.bert.convert_tokens_to_ids(*args, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def label_binarize(labels, classes):\n",
    "    # labels: np.array or tensor [batch, classes]\n",
    "    # classes: [..] list of classes\n",
    "    # weirdly,`sklearn.preprocessing.label_binarize` returns [1] or [0]\n",
    "    # instead of onehot ONLY when executing in this script!\n",
    "    vectors = [np.zeros(len(classes)) for _ in classes]\n",
    "    for i, label in enumerate(labels):\n",
    "        for j, c in enumerate(classes):\n",
    "            if c == label:\n",
    "                vectors[i][j] = 1\n",
    "    return np.array(vectors)\n",
    "    \n",
    "\n",
    "def label_inv_binarize(vectors, classes):\n",
    "    # labels: np.array or tensor [batch, classes]\n",
    "    # classes: [..] list of classes\n",
    "    # follows sklearn LabelBinarizer.inverse_transform()\n",
    "    # given all zeros, predicts label at index 0, instead of returning none!\n",
    "    # sklearn doesn't have functional API of inverse transform\n",
    "    labels = []\n",
    "    for each in vectors:\n",
    "        index = np.argmax(each)\n",
    "        labels.append(classes[index])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def padded_array(array, value=0):\n",
    "    # TODO: this does not do type checking.\n",
    "    # expects array to have fixed _number_ of dimensions\n",
    "    # resolve the shape of padded array\n",
    "    shape_index = {}\n",
    "    queue = [(array, 0)]\n",
    "    while queue:\n",
    "        subarr, dim = queue.pop(0)\n",
    "        shape_index[dim] = max(shape_index.get(dim, -1), len(subarr))\n",
    "        for x in subarr:\n",
    "            if isinstance(x, Iterable):\n",
    "                queue.append((x, dim+1))\n",
    "    shape = [shape_index[k] for k in range(max(shape_index) + 1)]\n",
    "\n",
    "    padded = np.ones(shape) * value\n",
    "    queue = [(array, [])]\n",
    "    while queue:\n",
    "        subarr, index = queue.pop(0)\n",
    "        for j, x in enumerate(subarr):\n",
    "            if isinstance(x, Iterable):\n",
    "                queue.append((x, index + [j]))\n",
    "            else:\n",
    "                padded[tuple(index + [j])] = x\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueDataset(D.Dataset):\n",
    "\n",
    "    def __init__(self, filename, schemas, tokenizer, token_indexer):\n",
    "        with open(filename) as f:\n",
    "            self.ds = json.load(f)\n",
    "        self.schemas = schemas\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexer = token_indexer\n",
    "        self.dialogues = []\n",
    "        self.default_padding = 0\n",
    "        for dial in self.ds:\n",
    "            fields = self.text_to_fields(dial)\n",
    "            self.dialogues.append(fields)\n",
    "        # cant' pickle these, and not required too\n",
    "        self.tokenizer = None\n",
    "        self.token_indexer = None\n",
    "        self.schemas = None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dialogues[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def field_dialogue_id(self, dialogue):\n",
    "        return {\"value\": dialogue[\"dialogue_id\"]}\n",
    "\n",
    "    def field_turn_speaker(self, turnid, dialogue):\n",
    "        return {\"value\": dialogue[\"turns\"][turnid][\"speaker\"]}\n",
    "    \n",
    "    def field_turn_utter(self, turnid, dialogue):\n",
    "        text = dialogue[\"turns\"][turnid][\"utterance\"]\n",
    "        tokens = self.tokenizer(text)\n",
    "        token_indices = self.token_indexer(tokens)\n",
    "        token_mask = [1] * len(tokens)\n",
    "        return {\"value\": text, \"tokens\": tokens, \"ids\": token_indices, \"mask\": token_mask}\n",
    "    \n",
    "    def field_turn_sys_utter(self, turnid, dialogue):\n",
    "        turn = dialogue[\"turns\"][turnid]\n",
    "        if turn[\"speaker\"] == \"SYSTEM\":\n",
    "            return self.field_turn_utter(turnid, dialogue)\n",
    "        \n",
    "    def field_turn_usr_utter(self, turnid, dialogue):\n",
    "        turn = dialogue[\"turns\"][turnid]\n",
    "        if turn[\"speaker\"] == \"USER\":\n",
    "            return self.field_turn_utter(turnid, dialogue)\n",
    "\n",
    "    def field_service(self, dialogue):\n",
    "        return {\"value\": dialogue[\"services\"]}\n",
    "\n",
    "    def field_service_desc(self, dialogue):\n",
    "        resp = dict(\n",
    "            value=[],\n",
    "            tokens=[],\n",
    "            ids=[],\n",
    "            mask=[]\n",
    "        )\n",
    "        for service in dialogue[\"services\"]:\n",
    "            desc = self.schemas.get_service_desc(service)\n",
    "            resp[\"value\"].append(desc)\n",
    "            resp[\"tokens\"].append(self.tokenizer(desc))\n",
    "            resp[\"ids\"].append(self.token_indexer(resp[\"tokens\"][-1]))\n",
    "            resp[\"mask\"].append([1] * len(resp[\"tokens\"][-1]))\n",
    "        return resp\n",
    "\n",
    "    def field_turn_service_exist(self, turnid, dialogue):\n",
    "        turn = dialogue[\"turns\"][turnid]\n",
    "        services = dialogue[\"services\"]\n",
    "        # order frames by dialog.services list, to establish one to one mappings across fields\n",
    "        sorted_frames = sorted(turn[\"frames\"], key=lambda x: services.index(x[\"service\"]))\n",
    "        exists_onehot = label_binarize([f[\"service\"] for f in sorted_frames], classes=services)\n",
    "        exists = np.sum(exists_onehot, axis=0) # eg: [1, 0, 1, 0]\n",
    "        return {\"ids\": exists, \"padding\": -1}\n",
    "        \n",
    "    def field_intent(self, dialogue):\n",
    "        return {\"value\": [self.schemas.get(s)[\"intent_name\"] for s in dialogue[\"services\"]]}\n",
    "\n",
    "    def field_intent_desc(self, dialogue):\n",
    "        resp = dict(\n",
    "            value=[],\n",
    "            tokens=[],\n",
    "            ids=[],\n",
    "            mask=[]\n",
    "        )\n",
    "        for service in dialogue[\"services\"]:\n",
    "            s_desc = [d for d in self.schemas.get(service)[\"intent_desc\"]]\n",
    "            s_tokens = [self.tokenizer(d) for d in s_desc]\n",
    "            s_ids = [self.token_indexer(d) for d in s_tokens]\n",
    "            s_mask = [[1] * len(d) for d in s_tokens]\n",
    "            resp[\"value\"].append(s_desc)\n",
    "            resp[\"tokens\"].append(s_tokens)\n",
    "            resp[\"ids\"].append(s_ids)\n",
    "            resp[\"mask\"].append(s_mask)\n",
    "        return resp\n",
    "\n",
    "    def field_turn_intent_exist(self, turnid, dialogue):\n",
    "        turn = dialogue[\"turns\"][turnid]\n",
    "        if turn[\"speaker\"] == \"USER\":\n",
    "            # maintain order of services; onehot per service\n",
    "            exists_onehot = OrderedDict()\n",
    "            for service in dialogue[\"services\"]:\n",
    "                exists_onehot[service] = None\n",
    "            \n",
    "            # fill encodings of existing services\n",
    "            # this _will_ be onehot assuming each service has only one intent!\n",
    "            for frame in turn[\"frames\"]:\n",
    "                service = frame[\"service\"]\n",
    "                all_intents = self.schemas.get(service)[\"intent_name\"]\n",
    "                intent = frame[\"state\"][\"active_intent\"]\n",
    "                encoding = label_binarize([intent], classes=all_intents)[0]\n",
    "                exists_onehot[service] = encoding\n",
    "            \n",
    "            # fill with empty encodings for remaining\n",
    "            for service in exists_onehot:\n",
    "                if exists_onehot[service] is None:\n",
    "                    all_intents = self.schemas.get(service)[\"intent_name\"]\n",
    "                    encoding = np.array([0] * len(all_intents))\n",
    "                    exists_onehot[service] = encoding\n",
    "                    \n",
    "            return {\"ids\": list(exists_onehot.values()), \"padding\": -1}\n",
    "\n",
    "    def field_turn_intent_changed(self, turnid, dialogue):\n",
    "        turn = dialogue[\"turns\"][turnid]\n",
    "        if turn[\"speaker\"] == \"USER\":\n",
    "            # one hot encoding of which intent exists.. in current and prev user turn\n",
    "            intent_exist = self.field_turn_intent_exist(turnid, dialogue)[\"ids\"]\n",
    "            prev_intent_exist = self.field_turn_intent_exist(max(0, turnid-2), dialogue)[\"ids\"]\n",
    "            assert len(intent_exist) == len(prev_intent_exist)\n",
    "            \n",
    "            # its an array of onehots.. one per service\n",
    "            changed = []\n",
    "            for curr, prev in zip(intent_exist, prev_intent_exist):\n",
    "                # at the first turn, assume intent-exist => intent-changed \n",
    "                if turnid == 0:\n",
    "                    changed_at_service = curr\n",
    "                else:\n",
    "                    changed_at_service = (curr != prev) * 1\n",
    "                changed.append(changed_at_service)\n",
    "            \n",
    "            return {\"ids\": changed, \"padding\": -1}\n",
    "\n",
    "    def field_slots(self, dialogue):\n",
    "        slot_list = []\n",
    "        for service in dialogue[\"services\"]:\n",
    "            slots = self.schemas.get(service)[\"slot_name\"]\n",
    "            slot_list.append(slots)\n",
    "        return {\"value\": slot_list}\n",
    "\n",
    "    def field_slots_desc(self, dialogue):\n",
    "        resp = dict(\n",
    "            value=[],\n",
    "            tokens=[],\n",
    "            ids=[],\n",
    "            mask=[]\n",
    "        )\n",
    "        for service in dialogue[\"services\"]:\n",
    "            s_desc = [d for d in self.schemas.get(service)[\"slot_desc\"]]\n",
    "            s_tokens = [self.tokenizer(d) for d in s_desc]\n",
    "            s_ids = [self.token_indexer(d) for d in s_tokens]\n",
    "            s_mask = [[1] * len(d) for d in s_tokens]\n",
    "            resp[\"value\"].append(s_desc)\n",
    "            resp[\"tokens\"].append(s_tokens)\n",
    "            resp[\"ids\"].append(s_ids)\n",
    "            resp[\"mask\"].append(s_mask)\n",
    "        return resp\n",
    "\n",
    "    def field_slots_iscat(self, dialogue):\n",
    "        iscat_list = []\n",
    "        for service in dialogue[\"services\"]:\n",
    "            iscat = [int(i) for i in self.schemas.get(service)[\"slot_iscat\"]]\n",
    "            iscat_list.append(iscat)\n",
    "        return {\"ids\": iscat_list, \"padding\": -1}\n",
    "\n",
    "    def field_num_turns(self, dialogue):\n",
    "        return {\"value\": len(dialogue[\"turns\"])}\n",
    "\n",
    "    def field_turn_num_frames(self, turnid, dialogue):\n",
    "        return {\"value\": len(dialogue[\"turns\"][turnid][\"frames\"])}\n",
    "\n",
    "    def text_to_fields(self, dialogue):\n",
    "        \"\"\"\n",
    "        fields = dict(\n",
    "            dialogue_id=None, # [Batch,]\n",
    "            num_turns=None, # [Batch,]\n",
    "            num_frames=[], # [Batch, Turn] equals to number of services per turn\n",
    "\n",
    "            # messages\n",
    "            speaker=[], # [Batch, Turn]\n",
    "            utter=[], # [Batch, Turn, Tokens]\n",
    "            sys_utter=[], # [Batch, Turn, Tokens] only system utters\n",
    "            usr_utter=[], # [Batch, Turn, Tokens] only user utters\n",
    "\n",
    "            # services\n",
    "            service=None, # [Batch, Service] all dialog services\n",
    "            service_desc=None, # [Batch, Service, Tokens] service descriptions\n",
    "            service_exist=[], # [Batch, Turn, Service] binarized\n",
    "            \n",
    "            # intents\n",
    "            intent=None, # [Batch, Service, Intent]\n",
    "            intent_desc=None, # [Batch, Service, Intent, Tokens]\n",
    "            intent_exist=[], # [Batch, Turn, Service, Intent]\n",
    "            intent_changed=[], # [Batch, Turn, Service]\n",
    "\n",
    "            # state slots\n",
    "            slots=None, # [Batch, Service, Slot]\n",
    "            slots_desc=None, # [Batch, Service, Slot, Tokens]\n",
    "            slots_iscat=None, # [Batch, Service, Slot]\n",
    "        )\n",
    "        \"\"\"\n",
    "        fields = {}\n",
    "        \n",
    "        # filter the field names in the instance\n",
    "        dial_field_funcs = []\n",
    "        turn_field_funcs = []\n",
    "        for attr in dir(self):\n",
    "            if attr.startswith(\"field_turn_\"):\n",
    "                turn_field_funcs.append(attr)\n",
    "            elif attr.startswith(\"field_\"):\n",
    "                dial_field_funcs.append(attr)\n",
    "                        \n",
    "        # fill dialogue level fields\n",
    "        for func in dial_field_funcs:\n",
    "            name = func.split(\"field_\", maxsplit=1)[-1]\n",
    "            resp = getattr(self, func)(dialogue)\n",
    "            resp[\"padding\"] = resp.get(\"padding\", self.default_padding)\n",
    "            fields[name] = resp\n",
    "\n",
    "        # fill turn level fields\n",
    "        for turnid in range(len(dialogue[\"turns\"])):\n",
    "            for func in turn_field_funcs:\n",
    "                name = func.split(\"field_turn_\", maxsplit=1)[-1]\n",
    "                resp = getattr(self, func)(turnid, dialogue) or {}\n",
    "                if name not in fields:\n",
    "                    fields[name] = {\"padding\": resp.get(\"padding\", self.default_padding)}\n",
    "                for k, v in resp.items():\n",
    "                    if k != \"padding\":\n",
    "                        fields[name][k] = fields[name].get(k, [])\n",
    "                        fields[name][k].append(v)\n",
    "                        \n",
    "        # combine the turn field ids and mask.. with default padding or the one given by func resp\n",
    "        for name, data in fields.items():\n",
    "            padding_value = data[\"padding\"]\n",
    "            for attr in [\"ids\", \"mask\"]:\n",
    "                if attr in data:\n",
    "                    data[attr] = padded_array(data[attr], padding_value)\n",
    "            \n",
    "        return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "bert_ = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = Tokenizer(bert_)\n",
    "token_indexer = TokenIndexer(bert_)\n",
    "\n",
    "train_schemas = Schemas(\"../data/train/schema.json\")\n",
    "test_schemas = Schemas(\"../data/dev/schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5cc82a350442d1b9581ce4257b503c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=127), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "train_dial_sets = []\n",
    "train_dial_files = sorted(glob.glob(\"../data/train/dialogues*.json\"))\n",
    "\n",
    "def worker(filename):\n",
    "    return DialogueDataset(filename, train_schemas, tokenizer, token_indexer)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    for ds in tqdm(executor.map(worker, train_dial_files), total=len(train_dial_files)):\n",
    "        train_dial_sets.append(ds)\n",
    "\n",
    "train_ds = D.ConcatDataset(train_dial_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b94112e8704f1faf56355de63354c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load test dataset\n",
    "test_dial_sets = []\n",
    "test_dial_files = sorted(glob.glob(\"../data/dev/dialogues*.json\"))\n",
    "\n",
    "def worker(filename):\n",
    "    return DialogueDataset(filename, test_schemas, tokenizer, token_indexer)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    for ds in tqdm(executor.map(worker, test_dial_files), total=len(test_dial_files)):\n",
    "        test_dial_sets.append(ds)\n",
    "\n",
    "test_ds = D.ConcatDataset(test_dial_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dialogue_mini_batcher(dialogues):\n",
    "    batch = {}\n",
    "    for dial in dialogues:\n",
    "        # populate the batch\n",
    "        for field, data in dial.items():\n",
    "            if field not in batch:\n",
    "                batch[field] = {}\n",
    "            for attr, val in data.items():\n",
    "                if attr == \"padding\":\n",
    "                    batch[field][attr] = val\n",
    "                else:\n",
    "                    batch[field][attr] = batch[field].get(attr, [])\n",
    "                    batch[field][attr].append(val)\n",
    "\n",
    "    # padding on field attributes\n",
    "    for field_name, data in batch.items():\n",
    "        for attr in [\"ids\", \"mask\"]:\n",
    "            if attr in data:\n",
    "                data[attr] = padded_array(data[attr], data[\"padding\"])\n",
    "                data[attr] = torch.tensor(data[attr], device=\"cpu\")\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def move_to_device(obj, device):\n",
    "    if type(obj) is list:\n",
    "        return [move_to_device(o, device) for o in obj]\n",
    "    elif type(obj) is dict:\n",
    "        return {k: move_to_device(v, device) for k, v in obj.items()}\n",
    "    elif type(obj) is torch.Tensor or isinstance(obj, nn.Module):\n",
    "        return obj.to(device)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import PretrainedBertEmbedder\n",
    "from allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogState(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.GRU(emb_size, emb_size, num_layers=2, batch_first=True)\n",
    "        self.register_buffer(\"state_l0\", None)\n",
    "\n",
    "    def forward(self, usr_utter, reset=False):\n",
    "        # get the state: reset or detach\n",
    "        if reset:\n",
    "            self.state_l0 = None\n",
    "        if self.state_l0 is not None:\n",
    "            self.state_l0 = self.state_l0.detach()\n",
    "\n",
    "        # encode utter [batch, seq, dir * embed], [layers * dir, batch, embed]\n",
    "        ht_usr, hw = self.l0(usr_utter, self.state_l0)\n",
    "\n",
    "        # update the state\n",
    "        self.state_l0 = hw\n",
    "        return hw\n",
    "\n",
    "\n",
    "class UserIntentPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # embedding layer for encoding text\n",
    "        self.emb = PretrainedBertEmbedder(\"bert-base-uncased\", requires_grad=False, top_layer_only=True)\n",
    "        \n",
    "        # encode utter and desc tokens\n",
    "        self.l0 = nn.GRU(self.emb.output_dim, self.emb.output_dim, batch_first=True, num_layers=2, bidirectional=True)\n",
    "        self.l1 = nn.GRU(self.emb.output_dim, self.emb.output_dim, batch_first=True, num_layers=2, bidirectional=True)\n",
    "        \n",
    "        # maintain dialog state\n",
    "        self.l2 = DialogState(2 * self.emb.output_dim)\n",
    "\n",
    "        # final FC\n",
    "        self.l3 = nn.Linear(2 * self.emb.output_dim, 1)\n",
    "        \n",
    "        # metrics\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "        # init weights: classifier's performance changes heavily on these\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.startswith(\"l0.\") or name.startswith(\"l1.\"):\n",
    "                print(\"Initializing bias/weights of \", name)\n",
    "                if \"weight\" in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    param.data.fill_(0.)\n",
    "\n",
    "    def get_metrics(self, reset):\n",
    "        return {\"acc\": self.accuracy.get_metric(reset)}\n",
    "\n",
    "    def get_score(self, usr_utter, usr_utter_h, intent_desc, intent_desc_h, context):\n",
    "        mat = torch.einsum(\"be,bitx,bx->bix\", usr_utter_h, intent_desc, context)\n",
    "        mat = self.l3(mat).squeeze(-1)\n",
    "        mat = torch.sigmoid(mat)\n",
    "        return mat\n",
    "\n",
    "    def forward(self, **batch):\n",
    "        turnid = batch[\"turnid\"]\n",
    "        serviceid = batch[\"serviceid\"]\n",
    "\n",
    "        usr_utter = batch[\"usr_utter\"][\"ids\"][:,turnid,:].long() # [batch, tokens]\n",
    "        usr_utter = self.emb(usr_utter) # [batch, tokens, emb]\n",
    "        usr_utter, usr_utter_h = self.l0(usr_utter) # [batch, seq, emb * dir] [layers * dir, batch, emb]\n",
    "        usr_utter_h = usr_utter_h[-1] # [batch, emb]\n",
    "\n",
    "        #  slice after embedding.. not working otherwise\n",
    "        intent_desc = batch[\"intent_desc\"][\"ids\"][:,serviceid,:].long() # [batch, intents, tokens]\n",
    "        shape = list(intent_desc.shape)\n",
    "\n",
    "        intent_desc = self.emb(intent_desc)  # [batch, intents, tokens, emb]\n",
    "        intent_desc, intent_desc_h = self.l1(torch.flatten(intent_desc, 1, 2)) # [batch, intents * tokens, emb * dir] [layers * dir, batch, emb]\n",
    "        intent_desc = intent_desc.reshape(shape + [-1]) # [batch, intents, tokens, emb * dir]\n",
    "        intent_desc_h = intent_desc_h[-1] # [batch, emb]\n",
    "        \n",
    "        # context\n",
    "        context = self.l2(usr_utter, reset=turnid==0) # [layers * dim, batch, emb * dir]\n",
    "        context = context[-1] # [batch, emb * dir]\n",
    "\n",
    "        \n",
    "        # compute prediction onehot\n",
    "        score = self.get_score(\n",
    "            usr_utter,\n",
    "            usr_utter_h,\n",
    "            intent_desc,\n",
    "            intent_desc_h,\n",
    "            context,\n",
    "        )\n",
    "\n",
    "        output = {\"score\": score}\n",
    "\n",
    "        if \"intent_exist\" in batch:\n",
    "            # target -- onehot. squared to keep padding=-1 unchanged after mult\n",
    "            target_score = (batch[\"intent_exist\"][\"ids\"][:,turnid,serviceid,:] * \n",
    "                            batch[\"intent_changed\"][\"ids\"][:,turnid,serviceid,:] ** 2) # [Batch, Intent]\n",
    "            target_values, target_labels = torch.max(target_score, -1) # [Batch,]\n",
    "\n",
    "            # update the accuracy counters. reset at every dialogue\n",
    "            mask = (target_values != -1).float()\n",
    "            self.accuracy(\n",
    "                score.float(),\n",
    "                target_labels.float(), \n",
    "                mask=mask,\n",
    "            )\n",
    "            \n",
    "#             with open(\"switch.txt\") as f:\n",
    "#                 switch = f.read()\n",
    "#                 if switch == \"1\":\n",
    "#                     # print(\"Target\", target_score.tolist())\n",
    "#                     # print(\"Pred: \", score.tolist())\n",
    "#                     print(\"Target\", torch.argmax(target_score, -1).tolist())\n",
    "#                     print(\"Pred: \", torch.argmax(score, -1).tolist())\n",
    "#                     print(\"Mask: \", mask.long().tolist())\n",
    "#                     print(\"Acc: \", self.accuracy.get_metric())\n",
    "#                     print(\"\\n\\n\")\n",
    "\n",
    "            # calculate loss\n",
    "            mask = (target_score != -1).float()\n",
    "            output[\"loss\"] = F.binary_cross_entropy(\n",
    "                score.float(),\n",
    "                target_score.float(),\n",
    "                weight=mask,\n",
    "            )\n",
    "#             mask = (target_score != -1).float()\n",
    "#             output[\"loss\"] = F.mse_loss(\n",
    "#                 score * mask,\n",
    "#                 target_score * mask,\n",
    "#                 reduction=\"sum\"\n",
    "#             )\n",
    "#             output[\"loss\"] /= target_score.shape[0]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogIterator(object):\n",
    "    \"\"\"A simple wrapper on DataLoader\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size, *args, **kw):\n",
    "        self.length = None\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.iterator = D.DataLoader(dataset, batch_size, *args, **kw)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.length is None:\n",
    "            self.length = 0\n",
    "            for dial in self.dataset:\n",
    "                self.length += dial[\"num_turns\"][\"value\"] + len(dial[\"service\"][\"value\"])\n",
    "            self.length = int(self.length / self.batch_size)\n",
    "        return self.length\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.iterator:\n",
    "            num_turns = batch[\"usr_utter\"][\"ids\"].shape[1]\n",
    "            num_services = batch[\"service_desc\"][\"ids\"].shape[1]\n",
    "            for turnid in range(num_turns):\n",
    "                for sid in range(num_services):\n",
    "                    inputs = dict(turnid=turnid, serviceid=sid)\n",
    "                    inputs.update(batch)\n",
    "                    yield inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(OrderedDict):\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        formatted = []\n",
    "        for k, v in self.items():\n",
    "            if type(v) is float:\n",
    "                v = round(v, 4)\n",
    "            formatted.append((k, v))\n",
    "        return \", \".join(\"{}: {}\".format(k, v) for k, v in formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataParallel(nn.DataParallel):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super(MyDataParallel, self).__init__(model)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            super(MyDataParallel, self).__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return getattr(self.module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "Initializing bias/weights of  l0.weight_ih_l0\n",
      "Initializing bias/weights of  l0.weight_hh_l0\n",
      "Initializing bias/weights of  l0.bias_ih_l0\n",
      "Initializing bias/weights of  l0.bias_hh_l0\n",
      "Initializing bias/weights of  l0.weight_ih_l0_reverse\n",
      "Initializing bias/weights of  l0.weight_hh_l0_reverse\n",
      "Initializing bias/weights of  l0.bias_ih_l0_reverse\n",
      "Initializing bias/weights of  l0.bias_hh_l0_reverse\n",
      "Initializing bias/weights of  l0.weight_ih_l1\n",
      "Initializing bias/weights of  l0.weight_hh_l1\n",
      "Initializing bias/weights of  l0.bias_ih_l1\n",
      "Initializing bias/weights of  l0.bias_hh_l1\n",
      "Initializing bias/weights of  l0.weight_ih_l1_reverse\n",
      "Initializing bias/weights of  l0.weight_hh_l1_reverse\n",
      "Initializing bias/weights of  l0.bias_ih_l1_reverse\n",
      "Initializing bias/weights of  l0.bias_hh_l1_reverse\n",
      "Initializing bias/weights of  l1.weight_ih_l0\n",
      "Initializing bias/weights of  l1.weight_hh_l0\n",
      "Initializing bias/weights of  l1.bias_ih_l0\n",
      "Initializing bias/weights of  l1.bias_hh_l0\n",
      "Initializing bias/weights of  l1.weight_ih_l0_reverse\n",
      "Initializing bias/weights of  l1.weight_hh_l0_reverse\n",
      "Initializing bias/weights of  l1.bias_ih_l0_reverse\n",
      "Initializing bias/weights of  l1.bias_hh_l0_reverse\n",
      "Initializing bias/weights of  l1.weight_ih_l1\n",
      "Initializing bias/weights of  l1.weight_hh_l1\n",
      "Initializing bias/weights of  l1.bias_ih_l1\n",
      "Initializing bias/weights of  l1.bias_hh_l1\n",
      "Initializing bias/weights of  l1.weight_ih_l1_reverse\n",
      "Initializing bias/weights of  l1.weight_hh_l1_reverse\n",
      "Initializing bias/weights of  l1.bias_ih_l1_reverse\n",
      "Initializing bias/weights of  l1.bias_hh_l1_reverse\n",
      "started training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c741ba653d114ffba2a8affb523e3f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2809), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c512f7cd0f3a4bd6bcc8708d4005014a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=416), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(model, optimizer, batch_size, num_epochs, train_ds, test_ds, device):\n",
    "    model = move_to_device(model, device)\n",
    "    model = nn.DataParallel(model)\n",
    "    for epoch in range(num_epochs):\n",
    "        # train\n",
    "        metrics = Metrics(run=\"train\")\n",
    "        model = model.train()\n",
    "        train_iter = DialogIterator(train_ds, batch_size, collate_fn=dialogue_mini_batcher)\n",
    "        train_pbar = tqdm(train_iter)\n",
    "        for batch in train_pbar:\n",
    "            batch = move_to_device(batch, device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**batch)\n",
    "            output[\"loss\"] = output[\"loss\"].mean()\n",
    "            output[\"loss\"].backward()\n",
    "            optimizer.step()\n",
    "            metrics[\"loss\"] = output[\"loss\"].item()\n",
    "            metrics[\"turn\"] = batch[\"turnid\"]\n",
    "            metrics.update(model.module.get_metrics(reset=True))\n",
    "            train_pbar.set_description(str(metrics))\n",
    "        \n",
    "        # test\n",
    "        metrics = Metrics(run=\"test\")\n",
    "        if test_ds:\n",
    "            test_iter = DialogIterator(test_ds, batch_size, collate_fn=dialogue_mini_batcher)\n",
    "            test_pbar = tqdm(test_iter)\n",
    "            with torch.no_grad():\n",
    "                model = model.eval()\n",
    "                for batch in test_pbar:\n",
    "                    batch = move_to_device(batch, device)\n",
    "                    output = model(**batch)\n",
    "                    metrics[\"loss\"] = output[\"loss\"].mean().item()\n",
    "                    metrics[\"turn\"] = batch[\"turnid\"]\n",
    "                    metrics.update(model.module.get_metrics(reset=True))\n",
    "                    test_pbar.set_description(str(metrics))\n",
    "            \n",
    "            \n",
    "print(\"loading model\")\n",
    "model = UserIntentPredictor()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "train_samples = train_ds #[train_ds[i] for i in range(100)]\n",
    "test_samples = test_ds\n",
    "\n",
    "print(\"started training\")\n",
    "train(\n",
    "    model=model,\n",
    "    optimizer=optim,\n",
    "    train_ds=train_samples,\n",
    "    test_ds=test_samples,\n",
    "    device=\"cuda\",\n",
    "    num_epochs=1,\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../data/model-00.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "190px",
    "left": "1950px",
    "right": "20px",
    "top": "120px",
    "width": "295px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
